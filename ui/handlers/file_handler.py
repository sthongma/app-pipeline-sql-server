"""File Operation Handlers"""
import os
import threading
import time
from datetime import datetime
from tkinter import messagebox, filedialog
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Tuple, Callable, Optional, Any
import pandas as pd
from utils.logger import setup_file_logging, cleanup_old_log_files
from config.json_manager import json_manager, get_log_folder
from performance_optimizations import PerformanceOptimizer
from utils.ui_helpers import format_elapsed_time
from ui.utils.button_utils import ProcessingFlag


class FileHandler:
    def __init__(
        self,
        file_service: Any,
        db_service: Any,
        file_mgmt_service: Any,
        log_callback: Callable[[str], None]
    ) -> None:
        """
        Initialize File Handler

        Args:
            file_service: File service instance
            db_service: Database service instance
            file_mgmt_service: File management service instance
            log_callback: Function to call for logging
        """
        self.file_service = file_service
        self.db_service = db_service
        self.file_mgmt_service = file_mgmt_service
        self.log: Callable[[str], None] = log_callback

        # Processing flags to prevent double-click
        self.is_checking: bool = False  # Flag to prevent multiple check operations
        self.is_uploading: bool = False  # Flag to prevent multiple upload operations

        # Initialize Performance Optimizer for parallel processing
        self.perf_optimizer = PerformanceOptimizer(log_callback=log_callback)
        self.max_workers: int = min(4, os.cpu_count() or 1)  # Number of parallel workers
    
    def browse_excel_path(self, save_callback):
        """Select folder for file search"""
        folder = filedialog.askdirectory()
        if folder:
            self.file_service.set_search_path(folder)
            save_callback(folder)
            self.log(f"üìÅ Input folder updated: {folder}")
            messagebox.showinfo("Success", f"Set search path for Excel files to\n{folder}")
    
    def run_check_thread(self, ui_callbacks):
        """Start file checking in separate thread"""
        # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏î‡∏ã‡πâ‡∏≥‡∏Ç‡∏ì‡∏∞‡∏Å‡∏≥‡∏•‡∏±‡∏á check ‡∏≠‡∏¢‡∏π‡πà
        if self.is_checking:
            self.log("‚ö†Ô∏è File scan is already in progress, please wait...")
            return

        # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ flag ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
        self.is_checking = True

        # ‡∏õ‡∏¥‡∏î‡∏õ‡∏∏‡πà‡∏°‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏î‡∏ã‡πâ‡∏≥ (‡∏ó‡∏≥‡πÉ‡∏ô main thread ‡∏Å‡πà‡∏≠‡∏ô start thread)
        try:
            ui_callbacks['disable_controls']()
        except Exception:
            pass

        thread = threading.Thread(target=self._check_files, args=(ui_callbacks,))
        thread.start()
    
    def _check_files(self, ui_callbacks):
        """Check files in specified path"""
        try:
            # ‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï UI
            ui_callbacks['reset_progress']()
            ui_callbacks['set_progress_status']("Starting file scan", "Scanning folders...")

            # ‡πÇ‡∏´‡∏•‡∏î‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡∏°‡πà
            self.file_service.load_settings()
            ui_callbacks['clear_file_list']()
            ui_callbacks['reset_select_all']()

            # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå Excel/CSV
            ui_callbacks['update_progress'](0.2, "Searching for files", "Scanning .xlsx and .csv files...")
            data_files = self.file_service.find_data_files()

            if not data_files:
                ui_callbacks['update_progress'](1.0, "Scan completed", "No .xlsx or .csv files found")
                ui_callbacks['update_status']("No .xlsx or .csv files found in the specified folder", True)
                self.log("‚ö†Ô∏è  No .xlsx or .csv files found in the specified folder")
                self.log("============  File scan completed ============")
                return

            found_files_count = 0
            total_files = len(data_files)

            for i, file in enumerate(data_files):
                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì progress ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (0.2 - 0.8)
                progress = 0.2 + (0.6 * (i / total_files))  # 20% - 80%
                ui_callbacks['update_progress'](progress, f"Checking file: {os.path.basename(file)}", f"File {i+1} of {total_files}")

                logic_type = self.file_service.detect_file_type(file)
                if logic_type:
                    found_files_count += 1
                    self.log(f"‚úÖ Found matching file: {os.path.basename(file)} [{logic_type}]")
                    ui_callbacks['add_file_to_list'](file, logic_type)

            if found_files_count > 0:
                ui_callbacks['update_progress'](1.0, "Scan completed", f"Found {found_files_count} matching files")
                ui_callbacks['update_status'](f"Found {found_files_count} matching files", False)
                ui_callbacks['enable_select_all']()
            else:
                ui_callbacks['update_progress'](1.0, "Scan completed", "No matching files found")
                ui_callbacks['update_status']("No matching files found", True)
                ui_callbacks['reset_select_all']()

            self.log("============  File scan completed ============")

        except Exception as e:
            self.log(f"‚ùå An error occurred while scanning files: {e}")
        finally:
            # ‡πÄ‡∏õ‡∏¥‡∏î‡∏õ‡∏∏‡πà‡∏°‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô
            ui_callbacks['enable_controls']()
            # ‡∏õ‡∏•‡πà‡∏≠‡∏¢ flag
            self.is_checking = False
    
    def confirm_upload(self, get_selected_files_callback, ui_callbacks):
        """‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å - with double-click protection"""
        # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏î‡∏ã‡πâ‡∏≥‡∏Ç‡∏ì‡∏∞‡∏Å‡∏≥‡∏•‡∏±‡∏á upload ‡∏≠‡∏¢‡∏π‡πà
        if self.is_uploading:
            self.log("‚ö†Ô∏è Upload is already in progress, please wait...")
            return

        selected = get_selected_files_callback()
        if not selected:
            messagebox.showwarning("No files", "Please select files to upload")
            return

        # ‡πÇ‡∏´‡∏•‡∏î‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡∏°‡πà‡∏Å‡πà‡∏≠‡∏ô‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
        self.log("üì• Loading latest settings before upload...")
        self.file_service.load_settings()

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        success, message = self.db_service.check_connection()
        if not success:
            messagebox.showerror(
                "Error",
                f"Cannot connect to database:\n{message}\n\nPlease check database settings first"
            )
            return

        # ‡∏ï‡∏±‡πâ‡∏á flag ‡∏Å‡πà‡∏≠‡∏ô‡πÅ‡∏™‡∏î‡∏á confirmation dialog
        self.is_uploading = True

        answer = messagebox.askyesno(
            "Confirm Upload",
            f"Are you sure you want to upload the selected {len(selected)} files?"
        )

        if answer:
            ui_callbacks['reset_progress']()
            ui_callbacks['disable_controls']()
            thread = threading.Thread(target=self._upload_selected_files_wrapper, args=(selected, ui_callbacks))
            thread.start()
        else:
            # ‡∏ñ‡πâ‡∏≤ cancel ‡πÉ‡∏´‡πâ‡∏õ‡∏•‡∏î‡∏•‡πá‡∏≠‡∏Å flag
            self.is_uploading = False
            self.log("Upload cancelled by user")

    def _upload_selected_files_wrapper(self, selected_files, ui_callbacks):
        """Wrapper to reset upload flag after upload completes"""
        try:
            self._upload_selected_files(selected_files, ui_callbacks)
        finally:
            # Always reset flag, even if an exception occurred
            self.is_uploading = False

    def _group_files_by_type(self, selected_files: List[Tuple]) -> Dict[str, List[Tuple]]:
        """‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÑ‡∏ü‡∏•‡πå‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó (logic_type)"""
        files_by_type: Dict[str, List[Tuple]] = {}
        for (file_path, logic_type), chk in selected_files:
            if logic_type not in files_by_type:
                files_by_type[logic_type] = []
            files_by_type[logic_type].append((file_path, chk))
        return files_by_type

    def _init_upload_stats(self, start_time: float) -> Dict[str, Any]:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á initial upload statistics structure"""
        return {
            'total_start_time': start_time,
            'by_type': {},
            'errors': [],
            'successful_files': 0,
            'failed_files': 0,
            'processed_file_list': []
        }

    def _init_type_stats(self, files_count: int, start_time: float) -> Dict[str, Any]:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á statistics structure ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå"""
        return {
            'start_time': start_time,
            'files_count': files_count,
            'successful_files': 0,
            'failed_files': 0,
            'errors': [],
            'individual_processing_time': 0,
            'successful_file_list': [],
            'failed_file_list': []
        }

    def _validate_single_file(self, file_info):
        """
        Validate a single file (helper function for parallel processing)

        Args:
            file_info: Tuple of (file_path, logic_type)

        Returns:
            Dict with validation results
        """
        file_path, logic_type = file_info
        result = {
            'file_path': file_path,
            'logic_type': logic_type,
            'success': False,
            'df': None,
            'error': None
        }

        try:
            file_start_time = time.time()

            # Check file size for optimization decision
            file_size = os.path.getsize(file_path)
            file_size_mb = file_size / (1024 * 1024)

            # Use performance optimizer for large files
            if file_size_mb > 50:
                self.log(f"üöÄ Using optimized reader for large file: {os.path.basename(file_path)} ({file_size_mb:.1f} MB)")

                # Determine file type
                if file_path.lower().endswith('.csv'):
                    file_type = 'csv'
                elif file_path.lower().endswith('.xls'):
                    file_type = 'excel_xls'
                else:
                    file_type = 'excel'

                success, df = self.perf_optimizer.read_large_file_chunked(file_path, file_type)
                if not success:
                    result['error'] = "Failed to read large file"
                    return result
            else:
                # Standard reading for smaller files
                # First check columns
                success, preview_result, columns_info = self.file_service.preview_file_columns(file_path, logic_type)
                if not success:
                    result['error'] = f"Column check failed: {preview_result}"
                    return result

                # Read full file
                success, read_result = self.file_service.read_excel_file(file_path, logic_type)
                if not success:
                    result['error'] = f"Failed to read file: {read_result}"
                    return result

                df = read_result

            # Successful validation
            result['success'] = True
            result['df'] = df

            file_processing_time = time.time() - file_start_time
            self.log(f"‚úÖ Validated: {os.path.basename(file_path)} ({file_processing_time:.1f}s)")

        except Exception as e:
            result['error'] = str(e)
            self.log(f"‚ùå Error validating {os.path.basename(file_path)}: {e}")

        return result

    def _upload_selected_files(self, selected_files, ui_callbacks):
        """
        ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏õ‡∏¢‡∏±‡∏á SQL Server (Enhanced with parallel processing)

        ‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô 2 phases:
        - Phase 1: ‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏•‡∏∞ validate ‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏ö‡∏ö parallel
        - Phase 2: ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà validated ‡πÅ‡∏•‡πâ‡∏ß
        """
        upload_start_time = time.time()

        # ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° stats
        files_by_type = self._group_files_by_type(selected_files)
        total_files = sum(len(files) for files in files_by_type.values())
        total_types = len(files_by_type)

        upload_stats = self._init_upload_stats(upload_start_time)

        # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
        ui_callbacks['set_progress_status']("Starting upload", f"Found {total_files} files from {total_types} types")

        # Phase 1: Read and validate all files with PARALLEL PROCESSING
        self.log("üìñ Phase 1: Reading and validating all files in parallel...")
        self.log(f"üöÄ Using {self.max_workers} parallel workers for optimal performance")
        all_validated_data = {}  # {logic_type: (combined_df, files_info, required_cols)}
        
        completed_types = 0
        processed_files = 0

        for logic_type, files in files_by_type.items():
            try:
                # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏±‡∏ö‡πÄ‡∏ß‡∏•‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ
                type_start_time = time.time()
                upload_stats['by_type'][logic_type] = self._init_type_stats(len(files), type_start_time)

                self.log(f"üìñ Validating files of type {logic_type} ({len(files)} files)")

                # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Progress Bar ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤
                progress = completed_types / total_types
                ui_callbacks['update_progress'](progress, f"Validating type {logic_type}", f"Type {completed_types + 1} of {total_types}")

                # ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
                all_dfs = []
                valid_files_info = []

                # PARALLEL FILE VALIDATION using ThreadPoolExecutor
                file_infos = [(file_path, logic_type) for file_path, chk in files]
                file_chks = {file_path: chk for file_path, chk in files}

                with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                    # Submit all validation tasks
                    future_to_file = {
                        executor.submit(self._validate_single_file, file_info): file_info
                        for file_info in file_infos
                    }

                    # Collect results as they complete
                    for future in as_completed(future_to_file):
                        file_info = future_to_file[future]
                        file_path, _ = file_info

                        try:
                            processed_files += 1
                            file_progress = (processed_files - 1) / total_files

                            # Get validation result
                            validation_result = future.result()

                            if validation_result['success']:
                                df = validation_result['df']
                                all_dfs.append(df)
                                valid_files_info.append((file_path, file_chks[file_path]))
                                upload_stats['by_type'][logic_type]['successful_files'] += 1
                                upload_stats['by_type'][logic_type]['successful_file_list'].append(os.path.basename(file_path))

                                # Update UI
                                ui_callbacks['update_progress'](
                                    file_progress,
                                    f"‚úÖ Validated: {os.path.basename(file_path)}",
                                    f"File {processed_files} of {total_files}"
                                )
                            else:
                                error = validation_result['error']
                                self.log(f"‚ùå Validation failed for {os.path.basename(file_path)}: {error}")
                                upload_stats['by_type'][logic_type]['failed_files'] += 1
                                upload_stats['by_type'][logic_type]['failed_file_list'].append(os.path.basename(file_path))
                                upload_stats['by_type'][logic_type]['errors'].append(f"{os.path.basename(file_path)}: {error}")
                                upload_stats['failed_files'] += 1

                                # Update UI
                                ui_callbacks['update_progress'](
                                    file_progress,
                                    f"‚ùå Failed: {os.path.basename(file_path)}",
                                    f"File {processed_files} of {total_files}"
                                )

                        except Exception as e:
                            error_msg = f"An error occurred while validating file {os.path.basename(file_path)}: {e}"
                            self.log(f"‚ùå {error_msg}")
                            upload_stats['by_type'][logic_type]['failed_files'] += 1
                            upload_stats['by_type'][logic_type]['errors'].append(f"{os.path.basename(file_path)}: {str(e)}")
                            upload_stats['failed_files'] += 1

                # Calculate processing time for this type
                upload_stats['by_type'][logic_type]['individual_processing_time'] = time.time() - type_start_time
                
                if not all_dfs:
                    self.log(f"‚ùå No valid data from files of type {logic_type}")
                    completed_types += 1
                    continue
                
                # ‡∏£‡∏ß‡∏° DataFrame ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
                combined_df = pd.concat(all_dfs, ignore_index=True)
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                ui_callbacks['update_progress'](file_progress, f"Combining data for type {logic_type}", f"Combined {len(all_dfs)} files into {len(combined_df)} rows")
                
                # ‡πÉ‡∏ä‡πâ dtype ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
                required_cols = self.file_service.get_required_dtypes(logic_type)
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ required_cols ‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤
                if not required_cols:
                    self.log(f"‚ùå No data type configuration found for {logic_type}")
                    completed_types += 1
                    continue
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤
                if combined_df.empty:
                    self.log(f"‚ùå No valid data from files of type {logic_type}")
                    completed_types += 1
                    continue
                
                # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡πâ‡∏ß
                all_validated_data[logic_type] = (combined_df, valid_files_info, required_cols)
                self.log(f"‚úÖ Prepared {len(combined_df)} rows for type {logic_type}")
                    
                completed_types += 1
                
            except Exception as e:
                error_msg = f"An error occurred while validating files of type {logic_type}: {e}"
                self.log(f"‚ùå {error_msg}")
                upload_stats['by_type'][logic_type]['errors'].append(error_msg)
                completed_types += 1
        
        # Phase 2: Upload all validated data (with proper table clearing sequence)
        if all_validated_data:
            self.log("üì§ Phase 2: Uploading all validated data...")
            upload_count = 0
            total_uploads = len(all_validated_data)
            
            for logic_type, (combined_df, valid_files_info, required_cols) in all_validated_data.items():
                try:
                    # ‡∏à‡∏±‡∏ö‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô Phase 2 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ
                    phase2_start_time = time.time()
                    
                    upload_progress = upload_count / total_uploads
                    ui_callbacks['update_progress'](upload_progress, f"Uploading data for type {logic_type}", f"Upload {upload_count + 1} of {total_uploads}")
                    
                    self.log(f"üìä Uploading {len(combined_df)} rows for type {logic_type}")
                    
                    # Clear existing data only for the first upload of each table
                    success, message = self.db_service.upload_data(
                        combined_df, logic_type, required_cols, 
                        log_func=self.log, clear_existing=True
                    )
                    
                    if success:
                        self.log(f"‚úÖ {message}")

                        # ‡πÄ‡∏Å‡πá‡∏ö summary message ‡πÑ‡∏ß‡πâ‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏ô report
                        upload_stats['by_type'][logic_type]['summary_message'] = message

                        upload_stats['successful_files'] += len(valid_files_info)
                        for file_path, chk in valid_files_info:
                            ui_callbacks['disable_checkbox'](chk)
                            ui_callbacks['set_file_uploaded'](file_path)
                            # ‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏´‡∏•‡∏±‡∏á‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
                            try:
                                move_success, move_result = self.file_service.move_uploaded_files([file_path], [logic_type])
                                if move_success:
                                    for original_path, new_path in move_result:
                                        self.log(f"üì¶ Moved file to: {new_path}")
                                else:
                                    self.log(f"‚ùå Could not move file: {move_result}")
                            except Exception as move_error:
                                self.log(f"‚ùå An error occurred while moving file: {move_error}")
                    else:
                        # ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏≤‡∏Å‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÑ‡∏°‡πà‡∏û‡∏¥‡∏°‡∏û‡πå‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
                        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ message ‡πÄ‡∏õ‡πá‡∏ô dict ‡∏´‡∏£‡∏∑‡∏≠ string
                        if isinstance(message, dict):
                            summary = message.get('summary', 'Upload failed')
                            validation_issues = message.get('issues', [])
                            validation_warnings = message.get('warnings', [])

                            self.log(f"‚ùå {summary}")
                            upload_stats['by_type'][logic_type]['errors'].append(f"Database upload failed: {summary}")
                            upload_stats['by_type'][logic_type]['validation_details'] = {
                                'issues': validation_issues,
                                'warnings': validation_warnings
                            }
                        else:
                            self.log(f"‚ùå {message}")
                            upload_stats['by_type'][logic_type]['errors'].append(f"Database upload failed: {message}")

                        upload_stats['failed_files'] += len(valid_files_info)

                        # ‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å successful_file_list ‡πÑ‡∏õ‡∏¢‡∏±‡∏á failed_file_list
                        for file_path, chk in valid_files_info:
                            filename = os.path.basename(file_path)
                            # ‡∏•‡∏ö‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å successful_file_list ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
                            if filename in upload_stats['by_type'][logic_type]['successful_file_list']:
                                upload_stats['by_type'][logic_type]['successful_file_list'].remove(filename)
                                upload_stats['by_type'][logic_type]['successful_files'] -= 1
                            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô failed_file_list
                            if filename not in upload_stats['by_type'][logic_type]['failed_file_list']:
                                upload_stats['by_type'][logic_type]['failed_file_list'].append(filename)
                                upload_stats['by_type'][logic_type]['failed_files'] += 1
                        
                    upload_count += 1
                    
                    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏ß‡∏•‡∏≤ Phase 2 ‡πÅ‡∏•‡∏∞‡∏£‡∏ß‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô individual_processing_time
                    phase2_time = time.time() - phase2_start_time
                    if 'individual_processing_time' in upload_stats['by_type'][logic_type]:
                        upload_stats['by_type'][logic_type]['individual_processing_time'] += phase2_time
                        upload_stats['by_type'][logic_type]['processing_time'] = upload_stats['by_type'][logic_type]['individual_processing_time']
                    
                except Exception as e:
                    error_msg = f"An error occurred while uploading data for type {logic_type}: {e}"
                    self.log(f"‚ùå {error_msg}")
                    upload_stats['by_type'][logic_type]['errors'].append(error_msg)
                    upload_count += 1
                    
                    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏ß‡∏•‡∏≤ Phase 2 ‡πÅ‡∏°‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡∏£‡∏ß‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô individual_processing_time
                    phase2_time = time.time() - phase2_start_time
                    if 'individual_processing_time' in upload_stats['by_type'][logic_type]:
                        upload_stats['by_type'][logic_type]['individual_processing_time'] += phase2_time
                        upload_stats['by_type'][logic_type]['processing_time'] = upload_stats['by_type'][logic_type]['individual_processing_time']
        else:
            self.log("‚ùå No validated data to upload")
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏ß‡∏•‡∏≤‡∏£‡∏ß‡∏°
        total_upload_time = time.time() - upload_start_time
        upload_stats['total_time'] = total_upload_time
        
        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï progress ‡πÄ‡∏õ‡πá‡∏ô 100% ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô
        ui_callbacks['update_progress'](1.0, "Upload completed", f"Processed {total_files} files successfully")
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏£‡∏∏‡∏õ
        self._display_upload_summary(upload_stats, total_files)
        
        # ‡πÄ‡∏õ‡∏¥‡∏î‡∏õ‡∏∏‡πà‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤
        ui_callbacks['enable_controls']()
    
    def _display_processing_summary(self, stats, total_files, operation_type="Upload"):
        """
        ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• (‡πÉ‡∏ä‡πâ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Upload ‡πÅ‡∏•‡∏∞ Auto Process)

        Args:
            stats: Dictionary containing processing statistics
            total_files: Total number of files processed
            operation_type: Type of operation ("Upload" or "Processing")
        """
        separator = "=" * 42
        self.log(f"{separator[:9]} {operation_type} Summary Report {separator[:9]}")

        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å log ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
        self._auto_export_logs()

        # ‡πÄ‡∏ß‡∏•‡∏≤‡∏£‡∏ß‡∏°
        total_time = stats.get('total_time', 0)
        time_str = format_elapsed_time(total_time)

        self.log(f"üìä Total {operation_type} Time: {time_str}")
        self.log(f"üìÅ Total Files Processed: {total_files}")

        # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 0
        successful_files = stats.get('successful_files', 0)
        failed_files = stats.get('failed_files', 0)

        if successful_files > 0:
            self.log(f"‚úÖ Successful: {successful_files}")
        if failed_files > 0:
            self.log(f"‚ùå Failed: {failed_files}")

        # ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå
        if stats.get('by_type'):
            self._display_file_type_details(stats, operation_type)

        # ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
        self._display_final_summary(stats, total_files, operation_type)

        self.log("=" * 42)

    def _display_file_type_details(self, stats, operation_type):
        """‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå"""
        self.log("")
        self.log("üìã Details by File Type (Occupation):")
        self.log("-" * 50)

        for file_type, type_stats in stats['by_type'].items():
            # ‡πÅ‡∏™‡∏î‡∏á header ‡∏Ç‡∏≠‡∏á file type
            type_time = type_stats.get('processing_time', 0)
            type_time_str = format_elapsed_time(type_time)

            self.log(f"üè∑Ô∏è  Occupation: {file_type}")
            self.log(f"   ‚è±Ô∏è  Processing Time: {type_time_str}")
            self.log(f"   üìÅ Total Files: {type_stats.get('files_count', 0)}")

            # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
            self._display_successful_files(type_stats, operation_type)

            # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß
            self._display_failed_files(type_stats)

            self.log("")

    def _display_successful_files(self, type_stats, operation_type):
        """‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à"""
        successful = type_stats.get('successful_files', 0)
        if successful <= 0:
            return

        self.log(f"   ‚úÖ Successful: {successful}")

        # ‡πÅ‡∏™‡∏î‡∏á summary message (‡∏°‡∏µ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Upload operation)
        summary_message = type_stats.get('summary_message', '')
        if summary_message and operation_type == "Upload":
            self.log(f"   üìã Upload Summary:")
            self.log(f"      {summary_message}")

        # ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
        successful_file_list = type_stats.get('successful_file_list', [])
        if successful_file_list:
            self.log(f"      Files:")
            for filename in successful_file_list:
                self.log(f"         ‚Ä¢ {filename}")

    def _display_failed_files(self, type_stats):
        """‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß"""
        failed = type_stats.get('failed_files', 0)
        if failed <= 0:
            return

        self.log(f"   ‚ùå Failed: {failed}")

        failed_file_list = type_stats.get('failed_file_list', [])
        errors = type_stats.get('errors', [])
        validation_details = type_stats.get('validation_details', {})

        if failed_file_list:
            self.log(f"      Files with error details:")
            for filename in failed_file_list:
                self.log(f"         ‚Ä¢ {filename}")
                # ‡∏´‡∏≤ error ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ
                matching_errors = [
                    err for err in errors
                    if filename in err or 'upload failed' in err.lower()
                ]
                if matching_errors:
                    for err in matching_errors:
                        error_detail = err.split(': ', 1)[-1] if ': ' in err else err
                        self.log(f"           ‚Ü≥ {error_detail}")
                else:
                    self.log(f"           ‚Ü≥ Unknown error")

        # ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î validation issues
        if validation_details:
            self._display_validation_details(validation_details)

    def _display_validation_details(self, validation_details):
        """‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î validation issues ‡πÅ‡∏•‡∏∞ warnings"""
        issues = validation_details.get('issues', [])
        warnings = validation_details.get('warnings', [])

        if issues:
            self.log(f"      Validation Issues (Serious):")
            for issue in issues:
                column = issue.get('column', 'Unknown')
                error_count = issue.get('error_count', 0)
                percentage = issue.get('percentage', 0)
                examples = issue.get('examples', '')
                self.log(f"         ‚ùå {column}: {error_count:,} invalid rows ({percentage}%) Examples: {examples}")

        if warnings:
            self.log(f"      Validation Warnings:")
            for warning in warnings:
                column = warning.get('column', 'Unknown')
                error_count = warning.get('error_count', 0)
                percentage = warning.get('percentage', 0)
                examples = warning.get('examples', '')
                self.log(f"         ‚ö†Ô∏è  {column}: {error_count:,} invalid rows ({percentage}%) Examples: {examples}")

    def _display_final_summary(self, stats, total_files, operation_type):
        """‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢"""
        success_rate = 0
        if total_files > 0:
            success_rate = (stats.get('successful_files', 0) / total_files) * 100

        self.log("üìà Summary:")
        self.log(f"   Success Rate: {success_rate:.1f}%")

        if stats.get('failed_files', 0) > 0:
            action = "upload" if operation_type == "Upload" else "process"
            self.log(f"   ‚ö†Ô∏è  Some files failed to {action}. Check the errors above for details.")
        else:
            action = "uploaded" if operation_type == "Upload" else "processed"
            self.log(f"   üéâ All files {action} successfully!")

    def _display_upload_summary(self, upload_stats, total_files):
        """‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î (wrapper for backward compatibility)"""
        self._display_processing_summary(upload_stats, total_files, "Upload")
    
    def _display_auto_process_summary(self, process_stats, total_files):
        """‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ (wrapper for backward compatibility)"""
        self._display_processing_summary(process_stats, total_files, "Processing")
    
    def start_auto_process(self, load_input_folder_callback, column_settings):
        """‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ (‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå)"""
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        input_folder_path = load_input_folder_callback()
        if not input_folder_path or not os.path.isdir(input_folder_path):
            messagebox.showerror(
                "Error",
                f"Invalid source folder: {input_folder_path}\n\nPlease select a source folder first"
            )
            return

        # ‡πÇ‡∏´‡∏•‡∏î‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡∏°‡πà‡∏Å‡πà‡∏≠‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
        self.file_service.load_settings()

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        success, message = self.db_service.check_connection()
        if not success:
            messagebox.showerror(
                "Error",
                f"Cannot connect to database:\n{message}\n\nPlease check database settings first"
            )
            return


        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå
        if not column_settings:
            messagebox.showerror(
                "Error",
                "No file type configuration found\n\nPlease go to Settings tab and add file types first"
            )
            return

        # ‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
        result = messagebox.askyesno(
            "Confirm Auto Processing",
            f"Will perform auto processing in folder:\n{input_folder_path}\n\n"
            "Processing steps:\n"
            "1. Find all data files\n"
            "2. Process and upload all files\n"
            "Do you want to proceed?"
        )

        if not result:
            return

        return input_folder_path  # Return path for further processing
    
    def run_auto_process(self, folder_path, ui_callbacks):
        """‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡πÉ‡∏ô thread ‡πÅ‡∏¢‡∏Å"""
        try:
            # ‡∏õ‡∏¥‡∏î‡∏õ‡∏∏‡πà‡∏°‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
            ui_callbacks['disable_controls']()
            
            # ‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï progress bar ‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
            ui_callbacks['reset_progress']()
            ui_callbacks['set_progress_status']("Starting auto processing", "Preparing system...")
            
            self.log("ü§ñ Starting auto processing")
            self.log(f"üìÅ Source folder: {folder_path}")
            
            # === ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏±‡∏Å ===
            self.log("========= Processing files ==========")
            self._auto_process_main_files(folder_path, ui_callbacks)
            
            self.log("==== Auto processing completed ======") 
            ui_callbacks['update_progress'](1.0, "Auto processing completed", "All steps completed successfully")
            messagebox.showinfo("Success", "Auto processing completed successfully")
            
        except Exception as e:
            self.log(f"‚ùå An error occurred during auto processing: {e}")
            messagebox.showerror("Error", f"An error occurred: {e}")
        finally:
            # ‡πÄ‡∏õ‡∏¥‡∏î‡∏õ‡∏∏‡πà‡∏°‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤
            ui_callbacks['enable_controls']()
    
    def _record_file_error(self, process_stats, logic_type, file_path, error_msg, file_start_time):
        """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å error ‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß"""
        basename = os.path.basename(file_path)

        process_stats['by_type'][logic_type]['failed_files'] += 1
        process_stats['by_type'][logic_type]['failed_file_list'].append(basename)
        process_stats['by_type'][logic_type]['errors'].append(f"{basename}: {error_msg}")
        process_stats['failed_files'] += 1

        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ
        file_processing_time = time.time() - file_start_time
        process_stats['by_type'][logic_type]['individual_processing_time'] += file_processing_time

        self.log(f"‚ùå {error_msg}")

    def _record_file_success(self, process_stats, logic_type, file_path, file_start_time):
        """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå"""
        basename = os.path.basename(file_path)

        process_stats['by_type'][logic_type]['successful_files'] += 1
        process_stats['by_type'][logic_type]['successful_file_list'].append(basename)
        process_stats['successful_files'] += 1

        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ
        file_processing_time = time.time() - file_start_time
        process_stats['by_type'][logic_type]['individual_processing_time'] += file_processing_time

    def _process_single_file(self, file_path, logic_type, process_stats, ui_callbacks, file_index, total_files):
        """
        ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß - ‡∏≠‡πà‡∏≤‡∏ô validate ‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î

        Returns:
            bool: True if successful, False otherwise
        """
        file_start_time = time.time()
        basename = os.path.basename(file_path)

        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤
        progress = (file_index - 1) / total_files
        ui_callbacks['update_progress'](progress, f"Processing file: {basename}", f"File {file_index} of {total_files}")

        self.log(f"üìÅ Processing file: {basename}")

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏Å‡πà‡∏≠‡∏ô
        success, result, _ = self.file_service.preview_file_columns(file_path, logic_type)
        if not success:
            self._record_file_error(process_stats, logic_type, file_path, f"Column check failed: {result}", file_start_time)
            return False

        # ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏ï‡πá‡∏°
        success, result = self.file_service.read_excel_file(file_path, logic_type)
        if not success:
            self._record_file_error(process_stats, logic_type, file_path, f"Could not read file: {result}", file_start_time)
            return False

        df = result

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏á
        if df.empty:
            self._record_file_error(process_stats, logic_type, file_path, "File has no data", file_start_time)
            return False

        # ‡∏î‡∏∂‡∏á required columns
        required_cols = self.file_service.get_required_dtypes(logic_type)
        if not required_cols:
            self._record_file_error(process_stats, logic_type, file_path, f"No data type configuration found for {logic_type}", file_start_time)
            return False

        # ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        self.log(f"üìä Uploading {len(df)} rows for type {logic_type}")
        success, message = self.db_service.upload_data(df, logic_type, required_cols, log_func=self.log, clear_existing=True)

        if success:
            self.log(f"‚úÖ Upload successful: {message}")
            self._record_file_success(process_stats, logic_type, file_path, file_start_time)

            # ‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏±‡∏á‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
            self._move_uploaded_file(file_path, logic_type)
            return True
        else:
            # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ upload failure
            self._handle_upload_failure(process_stats, logic_type, file_path, message, file_start_time)
            return False

    def _move_uploaded_file(self, file_path, logic_type):
        """‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß"""
        try:
            move_success, move_result = self.file_service.move_uploaded_files([file_path], [logic_type])
            if move_success:
                for original_path, new_path in move_result:
                    self.log(f"üì¶ Moved file to: {new_path}")
            else:
                self.log(f"‚ùå Could not move file: {move_result}")
        except Exception as move_error:
            self.log(f"‚ùå An error occurred while moving file: {move_error}")

    def _handle_upload_failure(self, process_stats, logic_type, file_path, message, file_start_time):
        """‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏ì‡∏µ upload ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß"""
        basename = os.path.basename(file_path)

        if isinstance(message, dict):
            summary = message.get('summary', 'Upload failed')
            validation_issues = message.get('issues', [])
            validation_warnings = message.get('warnings', [])

            error_msg = f"Upload failed: {summary}"
            process_stats['by_type'][logic_type]['validation_details'] = {
                'issues': validation_issues,
                'warnings': validation_warnings
            }
        else:
            error_msg = f"Upload failed: {message}"

        self._record_file_error(process_stats, logic_type, file_path, error_msg, file_start_time)

    def _detect_file_logic_type(self, file_path):
        """‡∏ï‡∏£‡∏ß‡∏à‡∏´‡∏≤ logic_type ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå"""
        logic_type = self.file_service.detect_file_type(file_path)
        if logic_type:
            return logic_type

        # ‡∏•‡∏≠‡∏á‡πÄ‡∏î‡∏≤‡∏à‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
        filename = os.path.basename(file_path).lower()
        for key in self.file_service.column_settings.keys():
            if key.lower() in filename:
                return key

        return None

    def _auto_process_main_files(self, folder_path, ui_callbacks):
        """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏±‡∏Å‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ (Refactored)"""
        try:
            process_start_time = time.time()

            # ‡∏ï‡∏±‡πâ‡∏á search path ‡πÅ‡∏•‡∏∞‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå
            self.file_service.set_search_path(folder_path)
            data_files = self.file_service.find_data_files()

            if not data_files:
                self.log("No data files found in source folder")
                return

            self.log(f"Found {len(data_files)} data files, starting processing...")

            total_files = len(data_files)
            process_stats = self._init_upload_stats(process_start_time)

            # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå
            for file_index, file_path in enumerate(data_files, start=1):
                try:
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏´‡∏≤ logic_type
                    logic_type = self._detect_file_logic_type(file_path)
                    if not logic_type:
                        error_msg = f"Could not identify file type: {os.path.basename(file_path)}"
                        self.log(f"‚ùå {error_msg}")
                        process_stats['failed_files'] += 1
                        process_stats['errors'].append(f"{os.path.basename(file_path)}: {error_msg}")
                        continue

                    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° stats ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö type ‡∏ô‡∏µ‡πâ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
                    if logic_type not in process_stats['by_type']:
                        process_stats['by_type'][logic_type] = self._init_type_stats(0, time.time())

                    process_stats['by_type'][logic_type]['files_count'] += 1
                    self.log(f"üìã Identified file type: {logic_type}")

                    # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ helper method
                    self._process_single_file(file_path, logic_type, process_stats, ui_callbacks, file_index, total_files)

                except Exception as e:
                    error_msg = f"An error occurred while processing {os.path.basename(file_path)}: {e}"
                    self.log(f"‚ùå {error_msg}")
                    if logic_type and logic_type in process_stats['by_type']:
                        process_stats['by_type'][logic_type]['failed_files'] += 1
                        process_stats['by_type'][logic_type]['errors'].append(f"{os.path.basename(file_path)}: {str(e)}")
                    process_stats['failed_files'] += 1
            
            # ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏£‡∏ß‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÅ‡∏¢‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó
            for logic_type in process_stats['by_type']:
                if 'individual_processing_time' in process_stats['by_type'][logic_type]:
                    process_stats['by_type'][logic_type]['processing_time'] = process_stats['by_type'][logic_type]['individual_processing_time']
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏ß‡∏•‡∏≤‡∏£‡∏ß‡∏°
            process_stats['total_time'] = time.time() - process_start_time
            
            # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï progress ‡πÄ‡∏õ‡πá‡∏ô 100% ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô
            ui_callbacks['update_progress'](1.0, "Processing completed", f"Successfully processed {successful_uploads} of {total_files} files")
            
            # ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏£‡∏∏‡∏õ
            self._display_auto_process_summary(process_stats, total_files)
            
            # ‡∏•‡πâ‡∏≤‡∏á list ‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏õ‡∏Å‡∏ï‡∏¥
            ui_callbacks['clear_file_list']()
            ui_callbacks['reset_select_all']()
            
        except Exception as e:
            self.log(f"‚ùå An error occurred while processing files: {e}")
    
    def _load_log_folder_from_config(self):
        """Load log folder path using JSONManager"""
        try:
            return get_log_folder()
        except Exception:
            return None

    def _auto_export_logs(self):
        """‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å log ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå log_pipeline ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πà‡∏≤"""
        try:
            # ‡πÇ‡∏´‡∏•‡∏î‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤
            app_settings = json_manager.load('app_settings')

            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            auto_export = app_settings.get('auto_export_logs', True)
            if not auto_export:
                return

            # ‡∏≠‡πà‡∏≤‡∏ô log folder path ‡∏à‡∏≤‡∏Å log_folder_config.json
            log_folder_path = self._load_log_folder_from_config()
            if not log_folder_path or not os.path.exists(log_folder_path):
                self.log("‚ö†Ô∏è Cannot export logs: No valid log folder configured")
                return

            # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ file logging ‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å
            log_file_path = setup_file_logging(log_folder_path, enable_export=True)
            if log_file_path:
                self.log(f"üìã Log exported to: {log_file_path}")

                # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå log ‡πÄ‡∏Å‡πà‡∏≤ (‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πà‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô retention period)
                retention_days = app_settings.get('log_retention_days', 30)
                deleted_count = cleanup_old_log_files(log_folder_path, retention_days)

                # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£ cleanup ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå
                if deleted_count > 0:
                    self.log(f"üßπ Cleaned up {deleted_count} old log files (older than {retention_days} days)")
            else:
                self.log("‚ö†Ô∏è Failed to export log file")

        except Exception as e:
            self.log(f"‚ùå Error during auto export logs: {e}")
