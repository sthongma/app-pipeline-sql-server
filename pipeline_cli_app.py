"""
CLI Application ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PIPELINE_SQLSERVER

‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô command line ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏ö‡∏ö batch
"""

"""CLI Application ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PIPELINE_SQLSERVER"""

import argparse
import logging
import os
from typing import Optional

from config.settings import settings_manager
from constants import AppConstants, PathConstants
from services.database_service import DatabaseService
from services.file_management_service import FileManagementService
from services.file_service import FileService

# ‡∏ó‡∏≥‡πÉ‡∏´‡πâ working directory ‡πÄ‡∏õ‡πá‡∏ô root ‡∏Ç‡∏≠‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡πÄ‡∏™‡∏°‡∏≠
os.chdir(os.path.dirname(os.path.abspath(__file__)))

# Setup logging
logging.basicConfig(
    level=logging.INFO, 
    format=AppConstants.LOG_FORMAT
)

def load_last_path() -> Optional[str]:
    """
    ‡πÇ‡∏´‡∏•‡∏î search path ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå last_path.json (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ö GUI)
    
    Returns:
        Optional[str]: search path ‡∏´‡∏£‡∏∑‡∏≠ None ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ
    """
    try:
        import json
        with open('config/last_path.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get('last_path', None)
    except (FileNotFoundError, json.JSONDecodeError, KeyError):
        return None

def process_file(file_path: str, file_service: FileService, db_service: DatabaseService) -> None:
    """
    ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß: ‡∏≠‡πà‡∏≤‡∏ô ‡πÅ‡∏õ‡∏•‡∏á ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î ‡πÅ‡∏•‡∏∞‡∏¢‡πâ‡∏≤‡∏¢ (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô GUI)
    
    Args:
        file_path: ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
        file_service: ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå
        db_service: ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    """
    try:
        logging.info(f"Processing file: {file_path}")
        # 1. ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏ï‡∏£‡∏ß‡∏à logic_type ‡∏à‡∏≤‡∏Å header ‡∏Å‡πà‡∏≠‡∏ô (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô GUI)
        logic_type = file_service.detect_file_type(file_path)
        # 2. ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ fallback ‡πÄ‡∏î‡∏≤‡∏à‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
        if not logic_type:
            filename = os.path.basename(file_path)
            if hasattr(file_service, 'column_settings'):
                for key in file_service.column_settings.keys():
                    if key.lower() in filename.lower():
                        logic_type = key
                        break
        if not logic_type:
            logging.warning(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå (logic_type) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {os.path.basename(file_path)} ‡πÑ‡∏î‡πâ ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ")
            return
        logging.info(f"Determined file type as: {logic_type}")
        # 3. ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô GUI)
        success, df_or_msg = file_service.read_excel_file(file_path, logic_type)
        if not success:
            logging.error(f"{df_or_msg}")
            return
        df = df_or_msg
        # 4. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô GUI)
        success, result = file_service.validate_columns(df, logic_type)
        if not success:
            logging.error(f"{result}")
            return
        required_cols = file_service.get_required_dtypes(logic_type)
        # 5. ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        success, message = db_service.upload_data(df, logic_type, required_cols)
        if success:
            logging.info(f"Successfully uploaded data from {file_path} to table for logic type {logic_type}.")
            # Move file after upload (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô GUI)
            move_success, move_result = file_service.move_uploaded_files([file_path], [logic_type])
            if move_success:
                for original_path, new_path in move_result:
                    logging.info(f"Moved file: {original_path} -> {new_path}")
            else:
                logging.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå: {move_result}")
        else:
            logging.error(f"Failed to upload data for {file_path}. Reason: {message}")
    except Exception as e:
        logging.error(f"An unexpected error occurred while processing {file_path}: {e}", exc_info=True)


def archive_old_files_cli(args) -> None:
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πà‡∏≤‡πÑ‡∏õ‡πÄ‡∏Å‡πá‡∏ö archive
    
    Args:
        args: arguments ‡∏à‡∏≤‡∏Å argparse
    """
    logging.info("Starting file archiving process.")
    
    file_mgmt_service = FileManagementService()
    
    # ‡πÉ‡∏ä‡πâ path ‡∏à‡∏≤‡∏Å args ‡∏´‡∏£‡∏∑‡∏≠ default path
    source_path = args.src or os.path.join(os.getcwd(), 'Uploaded_Files')
    archive_path = args.dest or os.path.join('D:\\', 'Archived_Files')
    
    if not os.path.exists(source_path):
        logging.error(f"Source path does not exist: {source_path}")
        return
    
    logging.info(f"Archiving files older than {args.days} days from {source_path} to {archive_path}")
    
    try:
        result = file_mgmt_service.archive_old_files(
            source_path=source_path,
            archive_path=archive_path,
            days=args.days,
            delete_archive_days=args.delete_days
        )
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        if result["moved_files"]:
            logging.info(f"Moved {len(result['moved_files'])} files to archive")
            for src, dst in result["moved_files"]:
                logging.info(f"  {src} -> {dst}")
        
        if result["moved_dirs"]:
            logging.info(f"Moved {len(result['moved_dirs'])} empty directories to archive")
            for src, dst in result["moved_dirs"]:
                logging.info(f"  {src} -> {dst}")
        
        if result["deleted_files"]:
            logging.info(f"Deleted {len(result['deleted_files'])} old files from archive")
            for file_path in result["deleted_files"]:
                logging.info(f"  Deleted: {file_path}")
        
        if result["errors"]:
            for error in result["errors"]:
                logging.error(f"Error during archiving: {error}")
        
        logging.info("File archiving process completed.")
        
    except Exception as e:
        logging.error(f"An error occurred during file archiving: {e}", exc_info=True)


def merge_zip_excel_cli(args) -> None:
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏°‡πÑ‡∏ü‡∏•‡πå Excel ‡∏à‡∏≤‡∏Å ZIP files
    
    Args:
        args: arguments ‡∏à‡∏≤‡∏Å argparse
    """
    logging.info("Starting ZIP Excel merger process.")
    
    file_mgmt_service = FileManagementService()
    
    if not os.path.exists(args.folder):
        logging.error(f"Folder does not exist: {args.folder}")
        return
    
    def progress_callback(value, status):
        """Callback ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤"""
        logging.info(f"Progress: {value*100:.1f}% - {status}")
    
    try:
        result = file_mgmt_service.process_zip_excel_merger(
            folder_path=args.folder,
            progress_callback=progress_callback
        )
        
        if result["success"]:
            logging.info("ZIP Excel merger completed successfully!")
            
            if result["saved_files"]:
                logging.info(f"Saved {len(result['saved_files'])} merged files:")
                for filename, rows in result["saved_files"]:
                    logging.info(f"  {filename}: {rows} rows")
            
            if result["organized_folder"] and result["moved_files"]:
                logging.info(f"Moved {len(result['moved_files'])} ZIP files to: {result['organized_folder']}")
        else:
            logging.error("ZIP Excel merger failed.")
            
        if result["errors"]:
            for error in result["errors"]:
                logging.error(f"Error during ZIP Excel merger: {error}")
        
    except Exception as e:
        logging.error(f"An error occurred during ZIP Excel merger: {e}", exc_info=True)


def validate_source_path() -> Optional[str]:
    """
    ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÇ‡∏´‡∏•‡∏î path ‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏à‡∏≤‡∏Å last_path.json
    
    Returns:
        Optional[str]: path ‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏´‡∏≤‡∏Å‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á, None ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
    """
    last_path = load_last_path()
    if not last_path or not os.path.isdir(last_path):
        logging.error(f"‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ô last_path.json: {last_path}")
        logging.error("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡πÉ‡∏ô config/last_path.json")
        return None
    
    logging.info(f"‡πÉ‡∏ä‡πâ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏à‡∏≤‡∏Å last_path.json: {last_path}")
    return last_path


def process_zip_files_step(source_path: str) -> None:
    """
    ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏£‡∏ß‡∏°‡πÑ‡∏ü‡∏•‡πå Excel ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå ZIP
    
    Args:
        source_path (str): ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå ZIP
    """
    logging.info("=== ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏£‡∏ß‡∏°‡πÑ‡∏ü‡∏•‡πå ZIP ===")
    try:
        file_mgmt_service = FileManagementService()
        
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå ZIP ‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á (‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á subfolder)
        zip_files = []
        for root, dirs, files in os.walk(source_path):
            for file in files:
                if file.lower().endswith('.zip'):
                    zip_files.append(os.path.join(root, file))
        
        if zip_files:
            logging.info(f"‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå ZIP {len(zip_files)} ‡πÑ‡∏ü‡∏•‡πå ‡∏à‡∏∞‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏°‡πÑ‡∏ü‡∏•‡πå Excel")
            
            def progress_callback(value: float, status: str) -> None:
                logging.info(f"‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤: {value*100:.1f}% - {status}")
            
            result = file_mgmt_service.process_zip_excel_merger(
                folder_path=source_path,
                progress_callback=progress_callback
            )
            
            if result["success"]:
                logging.info("‚úÖ ‡∏£‡∏ß‡∏°‡πÑ‡∏ü‡∏•‡πå Excel ‡∏à‡∏≤‡∏Å ZIP ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
                if result["saved_files"]:
                    for filename, rows in result["saved_files"]:
                        logging.info(f"  ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å: {filename} ({rows} ‡πÅ‡∏ñ‡∏ß)")
            else:
                logging.warning("‚ö†Ô∏è ‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏°‡πÑ‡∏ü‡∏•‡πå ZIP ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡πÅ‡∏ï‡πà‡∏à‡∏∞‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡πà‡∏≠")
                
            if result["errors"]:
                for error in result["errors"]:
                    logging.warning(f"‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô ZIP merger: {error}")
        else:
            logging.info("‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå ZIP ‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ")
            
    except Exception as e:
        logging.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏°‡πÑ‡∏ü‡∏•‡πå ZIP: {e}")
        logging.info("‡∏à‡∏∞‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡πà‡∏≠‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏±‡∏Å")


def process_main_files_step(source_path: str) -> None:
    """
    ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏±‡∏Å (Excel/CSV)
    
    Args:
        source_path (str): ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå
    """
    logging.info("=== ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏±‡∏Å ===")
    
    # ‡πÉ‡∏ä‡πâ logging.info ‡πÄ‡∏õ‡πá‡∏ô log_callback ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CLI
    file_service = FileService(search_path=source_path, log_callback=logging.info)
    db_service = DatabaseService()

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô
    is_connected, message = db_service.check_connection()
    if not is_connected:
        logging.error(f"Database connection failed: {message}. Exiting.")
        return

    logging.info(f"Database connection successful. Searching for files in: {file_service.search_path}")

    try:
        data_files = file_service.find_data_files()

        if not data_files:
            logging.info("No data files found in the specified path.")
        else:
            logging.info(f"Found {len(data_files)} files to process.")
            for file_path in data_files:
                process_file(file_path, file_service, db_service)
        
        logging.info("‚úÖ ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏±‡∏Å‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")

    except Exception as e:
        logging.error(f"An error occurred during file processing: {e}", exc_info=True)


def archive_old_files_step(source_path: str) -> None:
    """
    ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πà‡∏≤‡πÑ‡∏õ‡πÄ‡∏Å‡πá‡∏ö archive
    
    Args:
        source_path (str): ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πà‡∏≤
    """
    logging.info("=== ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πà‡∏≤‡πÑ‡∏õ‡∏ñ‡∏±‡∏á‡∏Ç‡∏¢‡∏∞ ===")
    
    try:
        file_mgmt_service = FileManagementService()
        
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á‡πÉ‡∏ô‡πÑ‡∏î‡∏£‡πå D
        archive_path = PathConstants.DEFAULT_ARCHIVE_PATH
        
        logging.info(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πà‡∏≤‡∏°‡∏≤‡∏Å‡∏ß‡πà‡∏≤ {AppConstants.DEFAULT_ARCHIVE_DAYS} ‡∏ß‡∏±‡∏ô‡∏à‡∏≤‡∏Å {source_path} ‡πÑ‡∏õ‡∏¢‡∏±‡∏á {archive_path}")
        
        result = file_mgmt_service.archive_old_files(
            source_path=source_path,
            archive_path=archive_path,
            days=AppConstants.DEFAULT_ARCHIVE_DAYS,
            delete_archive_days=AppConstants.DEFAULT_DELETE_ARCHIVE_DAYS
        )
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        if result["moved_files"]:
            logging.info(f"‚úÖ ‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå {len(result['moved_files'])} ‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏õ‡∏¢‡∏±‡∏á archive")
            
        if result["moved_dirs"]:
            logging.info(f"‚úÖ ‡∏¢‡πâ‡∏≤‡∏¢‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ß‡πà‡∏≤‡∏á {len(result['moved_dirs'])} ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÑ‡∏õ‡∏¢‡∏±‡∏á archive")
            
        if result["deleted_files"]:
            logging.info(f"üóëÔ∏è ‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πà‡∏≤ {len(result['deleted_files'])} ‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏õ‡∏ñ‡∏±‡∏á‡∏Ç‡∏¢‡∏∞")
            
        if result["errors"]:
            for error in result["errors"]:
                logging.warning(f"‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå: {error}")
        
        logging.info("‚úÖ ‡∏Å‡∏≤‡∏£‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πà‡∏≤‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
        
    except Exception as e:
        logging.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πà‡∏≤: {e}", exc_info=True)


def main_cli() -> None:
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CLI application
    
    ‡∏£‡∏±‡∏ô‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö:
    1. ‡∏£‡∏ß‡∏°‡πÑ‡∏ü‡∏•‡πå ZIP ‡∏Å‡πà‡∏≠‡∏ô
    2. ‡∏£‡∏±‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏´‡∏•‡∏±‡∏Å
    3. ‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πà‡∏≤‡∏°‡∏≤‡∏Å‡∏ß‡πà‡∏≤ {AppConstants.DEFAULT_ARCHIVE_DAYS} ‡∏ß‡∏±‡∏ô‡πÑ‡∏õ‡∏ñ‡∏±‡∏á‡∏Ç‡∏¢‡∏∞
    """
    logging.info("Starting CLI application for file processing.")

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÇ‡∏´‡∏•‡∏î path ‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á
    source_path = validate_source_path()
    if not source_path:
        return

    # ‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô
    process_zip_files_step(source_path)
    process_main_files_step(source_path)
    archive_old_files_step(source_path)
    
    logging.info("=== üèÅ ‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô ===")


def main():
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ command line arguments"""
    parser = argparse.ArgumentParser(
        description='PIPELINE_SQLSERVER CLI - ‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô command line ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô:

  # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡∏õ‡∏Å‡∏ï‡∏¥ (default)
  python pipeline_cli_app.py

  # ‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πà‡∏≤‡πÑ‡∏õ‡πÄ‡∏Å‡πá‡∏ö archive
  python pipeline_cli_app.py archive --days 30 --src ./Uploaded_Files --dest D:/Archived_Files

  # ‡∏£‡∏ß‡∏°‡πÑ‡∏ü‡∏•‡πå Excel ‡∏à‡∏≤‡∏Å ZIP files
  python pipeline_cli_app.py merge-zip --folder ./path/to/zip/folder
        """
    )
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á subcommands
    subparsers = parser.add_subparsers(dest='command', help='‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ')
    
    # Subcommand ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö archive files
    archive_parser = subparsers.add_parser(
        'archive', 
        help='‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πà‡∏≤‡πÑ‡∏õ‡πÄ‡∏Å‡πá‡∏ö archive ‡πÅ‡∏•‡∏∞‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πà‡∏≤‡πÉ‡∏ô archive'
    )
    archive_parser.add_argument(
        '--days', 
        type=int, 
        default=30, 
        help='‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏¢‡πâ‡∏≤‡∏¢ (default: 30)'
    )
    archive_parser.add_argument(
        '--src', 
        type=str, 
        help='‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á (default: ./Uploaded_Files)'
    )
    archive_parser.add_argument(
        '--dest', 
        type=str, 
        help='‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á (default: D:/Archived_Files)'
    )
    archive_parser.add_argument(
        '--delete-days', 
        type=int, 
        default=AppConstants.DEFAULT_DELETE_ARCHIVE_DAYS, 
        help=f'‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô archive (default: {AppConstants.DEFAULT_DELETE_ARCHIVE_DAYS})'
    )
    
    # Subcommand ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ZIP Excel merger
    merge_parser = subparsers.add_parser(
        'merge-zip', 
        help='‡∏£‡∏ß‡∏°‡πÑ‡∏ü‡∏•‡πå Excel ‡∏à‡∏≤‡∏Å ZIP files'
    )
    merge_parser.add_argument(
        '--folder', 
        type=str, 
        required=True, 
        help='‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå ZIP (‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏∏)'
    )
    
    args = parser.parse_args()
    
    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ï‡∏≤‡∏° command ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
    if args.command == 'archive':
        archive_old_files_cli(args)
    elif args.command == 'merge-zip':
        merge_zip_excel_cli(args)
    else:
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ command ‡∏´‡∏£‡∏∑‡∏≠ command ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ô‡πÅ‡∏ö‡∏ö‡∏õ‡∏Å‡∏ï‡∏¥
        main_cli()


if __name__ == '__main__':
    main()
