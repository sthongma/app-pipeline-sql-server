"""
Performance Optimizations ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PIPELINE_SQLSERVER

‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á Performance ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà:
1. ‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏ö‡∏ö Chunking
2. ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏ö‡∏ö Parallel Processing
3. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Memory ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô
4. ‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á Progress ‡∏ó‡∏µ‡πà‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ç‡∏∂‡πâ‡∏ô
5. ‡∏Å‡∏≤‡∏£‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ
"""

import os
import gc
import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Optional, Tuple, Callable, Any
import pandas as pd
import logging

class PerformanceOptimizer:
    """‡∏Ñ‡∏•‡∏≤‡∏™‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á Performance"""
    
    def __init__(self, log_callback: Optional[Callable] = None):
        self.log_callback = log_callback or logging.info
        self.cancellation_token = threading.Event()
        self.chunk_size = 10000  # ‡∏Ç‡∏ô‡∏≤‡∏î chunk ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå
        self.max_workers = min(4, os.cpu_count() or 1)  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô worker threads
        
    def set_cancellation_token(self, token: threading.Event):
        """‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ cancellation token ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô"""
        self.cancellation_token = token
        
    def read_large_file_chunked(self, file_path: str, file_type: str = 'excel') -> Tuple[bool, pd.DataFrame]:
        """
        ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà‡πÅ‡∏ö‡∏ö chunked ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î memory
        
        Args:
            file_path: ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏ü‡∏•‡πå
            file_type: ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå ('excel', 'excel_xls', ‡∏´‡∏£‡∏∑‡∏≠ 'csv')
            
        Returns:
            Tuple[bool, pd.DataFrame]: (‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà, DataFrame)
        """
        try:
            file_size = os.path.getsize(file_path)
            file_size_mb = file_size / (1024 * 1024)
            
            self.log_callback(f"üìÇ Read File: {os.path.basename(file_path)} ({file_size_mb:.1f} MB)")
            
            if file_size_mb > 100:  # ‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà‡∏Å‡∏ß‡πà‡∏≤ 100MB
                self.log_callback(f"‚ö†Ô∏è Large File, Use Chunked Reading")
                return self._read_large_file_chunked(file_path, file_type)
            else:
                return self._read_small_file(file_path, file_type)
                
        except Exception as e:
            error_msg = f"‚ùå Error Reading File: {e}"
            self.log_callback(error_msg)
            return False, pd.DataFrame()
    
    def _read_small_file(self, file_path: str, file_type: str) -> Tuple[bool, pd.DataFrame]:
        """‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å‡πÅ‡∏ö‡∏ö‡∏õ‡∏Å‡∏ï‡∏¥"""
        try:
            if file_type == 'csv':
                df = pd.read_csv(file_path, header=0, encoding='utf-8')
            elif file_type == 'excel_xls':
                df = pd.read_excel(file_path, header=0, sheet_name=0, engine='xlrd')
            else:
                df = pd.read_excel(file_path, header=0, sheet_name=0, engine='openpyxl')
            
            self.log_callback(f"‚úÖ Read File Success: {len(df):,} rows, {len(df.columns)} columns")
            return True, df
            
        except Exception as e:
            self.log_callback(f"‚ùå Error Reading File: {e}")
            return False, pd.DataFrame()
    
    def _read_large_file_chunked(self, file_path: str, file_type: str) -> Tuple[bool, pd.DataFrame]:
        """‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà‡πÅ‡∏ö‡∏ö chunked"""
        try:
            chunks = []
            total_rows = 0
            
            if file_type == 'csv':
                # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Å‡πà‡∏≠‡∏ô
                # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÉ‡∏ä‡πâ UTF-8 ‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏•‡πâ‡∏ß fallback ‡πÄ‡∏õ‡πá‡∏ô cp874/latin1 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏Å‡πà‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏ô‡∏£‡∏´‡∏±‡∏™
                try:
                    total_rows = sum(1 for _ in open(file_path, 'r', encoding='utf-8')) - 1
                    encoding_used = 'utf-8'
                except UnicodeDecodeError:
                    try:
                        total_rows = sum(1 for _ in open(file_path, 'r', encoding='cp874')) - 1
                        encoding_used = 'cp874'
                    except UnicodeDecodeError:
                        total_rows = sum(1 for _ in open(file_path, 'r', encoding='latin1')) - 1
                        encoding_used = 'latin1'
                self.log_callback(f"üìä Total Rows: {total_rows:,} (encoding={encoding_used})")
                
                # ‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö chunk
                chunk_reader = pd.read_csv(file_path, header=0, encoding=encoding_used, chunksize=self.chunk_size)
                
                for i, chunk in enumerate(chunk_reader):
                    if self.cancellation_token.is_set():
                        self.log_callback("‚ùå Work Cancelled")
                        return False, pd.DataFrame()
                    
                    chunks.append(chunk)
                    processed_rows = (i + 1) * self.chunk_size
                    progress = min(processed_rows / total_rows, 1.0)
                    
                    self.log_callback(f"üìñ Read Chunk {i+1}: {len(chunk):,} rows ({progress*100:.1f}%)")
                    
                    # ‡∏õ‡∏•‡πà‡∏≠‡∏¢ memory ‡∏ó‡∏∏‡∏Å 10 chunks
                    if (i + 1) % 10 == 0:
                        gc.collect()
                        
            elif file_type == 'excel_xls':  # .xls file
                # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö .xls ‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà ‡πÉ‡∏ä‡πâ xlrd
                import xlrd
                
                workbook = xlrd.open_workbook(file_path)
                worksheet = workbook.sheet_by_index(0)
                
                # ‡∏≠‡πà‡∏≤‡∏ô header
                headers = []
                for col_idx in range(worksheet.ncols):
                    cell_value = worksheet.cell_value(0, col_idx)
                    headers.append(cell_value)
                
                # ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö chunk
                chunk_data = []
                for row_idx in range(1, worksheet.nrows):
                    if self.cancellation_token.is_set():
                        self.log_callback("‚ùå Work Cancelled")
                        return False, pd.DataFrame()
                    
                    row_data = []
                    for col_idx in range(worksheet.ncols):
                        cell_value = worksheet.cell_value(row_idx, col_idx)
                        row_data.append(cell_value)
                    
                    chunk_data.append(row_data)
                    
                    # ‡∏™‡∏£‡πâ‡∏≤‡∏á chunk ‡∏ó‡∏∏‡∏Å chunk_size ‡πÅ‡∏ñ‡∏ß
                    if len(chunk_data) >= self.chunk_size:
                        chunk_df = pd.DataFrame(chunk_data, columns=headers)
                        chunks.append(chunk_df)
                        chunk_data = []
                        
                        self.log_callback(f"üìñ Read Chunk {len(chunks)}: {len(chunk_df):,} rows")
                        
                        # ‡∏õ‡∏•‡πà‡∏≠‡∏¢ memory
                        gc.collect()
                
                # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö .xls
                if chunk_data:
                    chunk_df = pd.DataFrame(chunk_data, columns=headers)
                    chunks.append(chunk_df)
                        
            else:  # Excel .xlsx file
                # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Excel ‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà ‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö chunk ‡∏î‡πâ‡∏ß‡∏¢ openpyxl
                import openpyxl
                from openpyxl.utils import get_column_letter
                
                workbook = openpyxl.load_workbook(file_path, read_only=True)
                worksheet = workbook.active
                
                # ‡∏≠‡πà‡∏≤‡∏ô header
                headers = []
                for cell in worksheet[1]:
                    headers.append(cell.value)
                
                # ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö chunk
                chunk_data = []
                for row_idx in range(2, worksheet.max_row + 1):
                    if self.cancellation_token.is_set():
                        self.log_callback("‚ùå Work Cancelled")
                        workbook.close()
                        return False, pd.DataFrame()
                    
                    row_data = []
                    for col_idx in range(1, len(headers) + 1):
                        cell = worksheet.cell(row=row_idx, column=col_idx)
                        row_data.append(cell.value)
                    
                    chunk_data.append(row_data)
                    
                    # ‡∏™‡∏£‡πâ‡∏≤‡∏á chunk ‡∏ó‡∏∏‡∏Å chunk_size ‡πÅ‡∏ñ‡∏ß
                    if len(chunk_data) >= self.chunk_size:
                        chunk_df = pd.DataFrame(chunk_data, columns=headers)
                        chunks.append(chunk_df)
                        chunk_data = []
                        
                        self.log_callback(f"üìñ Read Chunk {len(chunks)}: {len(chunk_df):,} rows")
                        
                        # ‡∏õ‡∏•‡πà‡∏≠‡∏¢ memory
                        gc.collect()
                
                # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠
                if chunk_data:
                    chunk_df = pd.DataFrame(chunk_data, columns=headers)
                    chunks.append(chunk_df)
                
                if file_type != 'excel_xls':
                    workbook.close()
            
            # ‡∏£‡∏ß‡∏° chunks
            if chunks:
                self.log_callback("üîÑ Combining chunks...")
                df = pd.concat(chunks, ignore_index=True)
                del chunks  # ‡∏õ‡∏•‡πà‡∏≠‡∏¢ memory
                gc.collect()
                
                self.log_callback(f"‚úÖ Read File Success - {len(df):,} rows, {len(df.columns)} columns")
                return True, df
            else:
                self.log_callback("‚ùå No data in file")
                return False, pd.DataFrame()
                
        except Exception as e:
            self.log_callback(f"‚ùå Error Reading File: {e}")
            return False, pd.DataFrame()
    
    def process_dataframe_in_chunks(self, df: pd.DataFrame, chunk_size: int = 5000) -> List[pd.DataFrame]:
        """
        ‡πÅ‡∏ö‡πà‡∏á DataFrame ‡πÄ‡∏õ‡πá‡∏ô chunks ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
        
        Args:
            df: DataFrame ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á
            chunk_size: ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ chunk
            
        Returns:
            List[pd.DataFrame]: ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ DataFrame chunks
        """
        chunks = []
        total_rows = len(df)
        
        for i in range(0, total_rows, chunk_size):
            end_idx = min(i + chunk_size, total_rows)
            chunk = df.iloc[i:end_idx].copy()
            chunks.append(chunk)
            
            if self.cancellation_token.is_set():
                break
        
        return chunks
    
    def parallel_process_files(self, file_paths: List[str], process_func: Callable, 
                             progress_callback: Optional[Callable] = None) -> List[Tuple[bool, Any]]:
        """
        ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏ö‡∏ö parallel
        
        Args:
            file_paths: ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏ü‡∏•‡πå
            process_func: ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå
            progress_callback: callback ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤
            
        Returns:
            List[Tuple[bool, Any]]: ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
        """
        results = []
        completed = 0
        total_files = len(file_paths)
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # ‡∏™‡πà‡∏á‡∏á‡∏≤‡∏ô‡πÑ‡∏õ‡∏¢‡∏±‡∏á executor
            future_to_file = {
                executor.submit(process_func, file_path): file_path 
                for file_path in file_paths
            }
            
            # ‡∏£‡∏≠‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
            for future in as_completed(future_to_file):
                if self.cancellation_token.is_set():
                    self.log_callback("‚ùå Work Cancelled")
                    break
                
                file_path = future_to_file[future]
                try:
                    result = future.result()
                    results.append((True, result))
                except Exception as e:
                    self.log_callback(f"‚ùå Error Processing File: {os.path.basename(file_path)}: {e}")
                    results.append((False, str(e)))
                
                completed += 1
                if progress_callback:
                    progress = completed / total_files
                    progress_callback(progress, f"Processing File {completed}/{total_files}")
        
        return results
    
    def optimize_memory_usage(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ memory ‡∏Ç‡∏≠‡∏á DataFrame
        
        Args:
            df: DataFrame ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á
            
        Returns:
            pd.DataFrame: DataFrame ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß
        """
        try:
            initial_memory = df.memory_usage(deep=True).sum() / 1024 / 1024  # MB
            self.log_callback(f"üíæ Memory Start: {initial_memory:.2f} MB")
            
            # ‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á numeric columns
            for col in df.select_dtypes(include=['int64']).columns:
                df[col] = pd.to_numeric(df[col], downcast='integer')
            
            for col in df.select_dtypes(include=['float64']).columns:
                df[col] = pd.to_numeric(df[col], downcast='float')
            
            # ‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á object columns
            for col in df.select_dtypes(include=['object']).columns:
                if df[col].nunique() / len(df) < 0.5:  # ‡∏ñ‡πâ‡∏≤ unique values ‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ 50%
                    df[col] = df[col].astype('category')
            
            final_memory = df.memory_usage(deep=True).sum() / 1024 / 1024  # MB
            memory_saved = initial_memory - final_memory
            
            self.log_callback(f"üíæ Memory After Optimization: {final_memory:.2f} MB (Saved {memory_saved:.2f} MB)")
            
            return df
            
        except Exception as e:
            self.log_callback(f"‚ö†Ô∏è Cannot optimize memory: {e}")
            return df
    
    def create_progress_tracker(self, total_items: int, description: str = "") -> Callable:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á progress tracker ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤
        
        Args:
            total_items: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
            description: ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
            
        Returns:
            Callable: ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤
        """
        start_time = time.time()
        completed = 0
        
        def update_progress(items_completed: int = 1, custom_message: str = ""):
            nonlocal completed
            completed += items_completed
            
            if total_items > 0:
                progress = completed / total_items
                elapsed_time = time.time() - start_time
                
                if completed > 0:
                    estimated_total = elapsed_time / completed * total_items
                    remaining_time = estimated_total - elapsed_time
                    
                    message = custom_message or f"{description}: {completed}/{total_items}"
                    time_info = f" (‡πÄ‡∏´‡∏•‡∏∑‡∏≠ {remaining_time:.1f}s)"
                    
                    return progress, message + time_info
                else:
                    return 0.0, f"{description}: Start..."
            else:
                return 1.0, f"{description}: Finished"
        
        return update_progress
    
    def cleanup_memory(self):
        """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î memory"""
        gc.collect()
        self.log_callback("üßπ Cleaned up memory")


class LargeFileProcessor:
    """‡∏Ñ‡∏•‡∏≤‡∏™‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞"""
    
    def __init__(self, log_callback: Optional[Callable] = None):
        self.optimizer = PerformanceOptimizer(log_callback)
        self.log_callback = log_callback or logging.info
        
    def process_large_file(self, file_path: str, file_type: str, 
                          processing_steps: List[Callable]) -> Tuple[bool, pd.DataFrame]:
        """
        ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
        
        Args:
            file_path: ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏ü‡∏•‡πå
            file_type: ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå
            processing_steps: ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
            
        Returns:
            Tuple[bool, pd.DataFrame]: (‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà, DataFrame ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß)
        """
        try:
            # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå
            success, df = self.optimizer.read_large_file_chunked(file_path, file_type)
            if not success:
                return False, pd.DataFrame()
            
            # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á memory
            df = self.optimizer.optimize_memory_usage(df)
            
            # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ï‡∏≤‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
            for i, step_func in enumerate(processing_steps):
                if self.optimizer.cancellation_token.is_set():
                    self.log_callback("‚ùå Work Cancelled")
                    return False, pd.DataFrame()
                
                self.log_callback(f"üîÑ Step {i+1}/{len(processing_steps)}: {step_func.__name__}")
                df = step_func(df)
                
                # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î memory ‡∏´‡∏•‡∏±‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô
                self.optimizer.cleanup_memory()
            
            return True, df
            
        except Exception as e:
            self.log_callback(f"‚ùå Error Processing File: {e}")
            return False, pd.DataFrame()
    
    def set_cancellation_token(self, token: threading.Event):
        """‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ cancellation token"""
        self.optimizer.set_cancellation_token(token)


# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
def create_chunk_processor(chunk_size: int = 5000):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö chunk"""
    def process_in_chunks(df: pd.DataFrame, process_func: Callable) -> pd.DataFrame:
        """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• DataFrame ‡πÅ‡∏ö‡∏ö chunk"""
        results = []
        total_chunks = (len(df) + chunk_size - 1) // chunk_size
        
        for i in range(0, len(df), chunk_size):
            chunk = df.iloc[i:i+chunk_size].copy()
            processed_chunk = process_func(chunk)
            results.append(processed_chunk)
            
            # ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤
            chunk_num = (i // chunk_size) + 1
            logging.info(f"üìä Processing chunk {chunk_num}/{total_chunks}")
        
        return pd.concat(results, ignore_index=True)
    
    return process_in_chunks


def estimate_processing_time(file_size_mb: float, processing_type: str = 'standard') -> float:
    """
    ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡πÄ‡∏ß‡∏•‡∏≤‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
    
    Args:
        file_size_mb: ‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô MB
        processing_type: ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
        
    Returns:
        float: ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£ (‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)
    """
    # ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÇ‡∏î‡∏¢‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì (MB/‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)
    rates = {
        'fast': 10.0,      # 10 MB/‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
        'standard': 5.0,   # 5 MB/‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
        'slow': 2.0        # 2 MB/‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
    }
    
    rate = rates.get(processing_type, rates['standard'])
    estimated_time = file_size_mb / rate
    
    return estimated_time


def format_file_size(size_bytes: int) -> str:
    """‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢"""
    if size_bytes == 0:
        return "0 B"
    
    size_names = ["B", "KB", "MB", "GB", "TB"]
    i = 0
    while size_bytes >= 1024 and i < len(size_names) - 1:
        size_bytes /= 1024.0
        i += 1
    
    return f"{size_bytes:.1f} {size_names[i]}"


def format_time(seconds: float) -> str:
    """‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢"""
    if seconds < 60:
        return f"{seconds:.1f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ"
    elif seconds < 3600:
        minutes = seconds / 60
        return f"{minutes:.1f} ‡∏ô‡∏≤‡∏ó‡∏µ"
    else:
        hours = seconds / 3600
        return f"{hours:.1f} ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á" 