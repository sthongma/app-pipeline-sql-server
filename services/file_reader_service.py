"""
File Reader Service ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PIPELINE_SQLSERVER

‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå Excel/CSV ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå
‡πÅ‡∏¢‡∏Å‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏à‡∏≤‡∏Å FileService ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÅ‡∏ï‡πà‡∏•‡∏∞ service ‡∏°‡∏µ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
"""

import json
import os
import re
import threading
from typing import Optional, Dict, Any

import pandas as pd

from constants import PathConstants


class FileReaderService:
    """
    ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå
    
    ‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö:
    - ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå Excel/CSV
    - ‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå Excel/CSV
    - ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå (file type detection)
    - ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ column mapping
    """
    
    def __init__(self, search_path: Optional[str] = None, log_callback: Optional[callable] = None) -> None:
        """
        ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô FileReaderService
        
        Args:
            search_path (Optional[str]): ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå
            log_callback (Optional[callable]): ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á log
        """
        # ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏ö‡∏∏ path ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ Downloads ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤ default
        if search_path:
            self.search_path = search_path
        else:
            self.search_path = PathConstants.DEFAULT_SEARCH_PATH
        
        # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ log callback
        self.log_callback = log_callback if log_callback else print
        
        # Cache ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤
        self._settings_cache: Dict[str, Any] = {}
        self._cache_lock = threading.Lock()
        self._settings_loaded = False
        
        self.load_settings()
    
    def load_settings(self) -> None:
        """‡πÇ‡∏´‡∏•‡∏î‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"""
        if self._settings_loaded:
            return
            
        try:
            # ‡πÇ‡∏´‡∏•‡∏î‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
            settings_file = PathConstants.COLUMN_SETTINGS_FILE
            if os.path.exists(settings_file):
                with open(settings_file, 'r', encoding='utf-8') as f:
                    self.column_settings = json.load(f)
            else:
                self.column_settings = {}
            
            # ‡πÇ‡∏´‡∏•‡∏î‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            dtype_file = PathConstants.DTYPE_SETTINGS_FILE
            if os.path.exists(dtype_file):
                with open(dtype_file, 'r', encoding='utf-8') as f:
                    self.dtype_settings = json.load(f)
            else:
                self.dtype_settings = {}
                
            self._settings_loaded = True
            
        except Exception:
            self.column_settings = {}
            self.dtype_settings = {}
            self._settings_loaded = True

    def set_search_path(self, path):
        """‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå Excel"""
        self.search_path = path

    def find_data_files(self):
        """‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå Excel (.xlsx, .xls) ‡πÅ‡∏•‡∏∞ CSV ‡πÉ‡∏ô path ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û)"""
        try:
            # ‡πÉ‡∏ä‡πâ os.scandir ‡πÅ‡∏ó‡∏ô glob ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß
            xlsx_files = []
            xls_files = []
            csv_files = []
            
            with os.scandir(self.search_path) as entries:
                for entry in entries:
                    if entry.is_file():
                        name_lower = entry.name.lower()
                        if name_lower.endswith('.xlsx'):
                            xlsx_files.append(entry.path)
                        elif name_lower.endswith('.xls'):
                            xls_files.append(entry.path)
                        elif name_lower.endswith('.csv'):
                            csv_files.append(entry.path)
            
            return xlsx_files + xls_files + csv_files
        except Exception:
            return []

    def standardize_column_name(self, col_name):
        """‡πÅ‡∏õ‡∏•‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô"""
        if pd.isna(col_name):
            return ""
        name = str(col_name).strip().lower()
        name = re.sub(r'[\s\W]+', '_', name)
        return name.strip('_')

    def get_column_name_mapping(self, file_type):
        """‡∏£‡∏±‡∏ö mapping ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå {original: new} ‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå"""
        if not file_type or file_type not in self.column_settings:
            return {}
        return self.column_settings[file_type]

    def normalize_col(self, col):
        """‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£ normalize column (‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á)"""
        if pd.isna(col):
            return ""
        
        # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô string ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡∏±‡πâ‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        normalized = str(col).strip()
        
        # ‡∏•‡∏ö zero-width characters ‡πÅ‡∏•‡∏∞ invisible characters
        normalized = normalized.replace('\u200b', '')  # Zero Width Space
        normalized = normalized.replace('\u200c', '')  # Zero Width Non-Joiner
        normalized = normalized.replace('\u200d', '')  # Zero Width Joiner
        normalized = normalized.replace('\ufeff', '')  # Byte Order Mark
        
        # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô lowercase ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
        normalized = normalized.lower()
        
        # ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á - ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô‡πÅ‡∏ï‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß
        normalized = ' '.join(normalized.split())
        
        return normalized

    def detect_file_type(self, file_path):
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå (‡πÅ‡∏ö‡∏ö dynamic, normalize header) ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á xlsx/xls/csv ‡πÅ‡∏•‡∏∞ identity mapping"""
        try:
            if not self.column_settings:
                return None

            # ‡∏≠‡πà‡∏≤‡∏ô‡∏´‡∏±‡∏ß‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏î‡∏≤‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå
            if file_path.lower().endswith('.csv'):
                df_peek = pd.read_csv(file_path, header=None, nrows=2, encoding='utf-8')
            elif file_path.lower().endswith('.xls'):
                # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå .xls ‡πÉ‡∏ä‡πâ xlrd engine
                df_peek = pd.read_excel(file_path, header=None, nrows=2, engine='xlrd')
            else:
                # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå .xlsx
                df_peek = pd.read_excel(file_path, header=None, nrows=2)

            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ó‡∏∏‡∏Å header row ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ
            for row in range(min(2, df_peek.shape[0])):
                header_row = set(self.normalize_col(col) for col in df_peek.iloc[row].values if not pd.isna(col))
                
                # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ header ‡πÉ‡∏ô row ‡∏ô‡∏µ‡πâ ‡∏Ç‡πâ‡∏≤‡∏°
                if not header_row:
                    continue
                
                # ‡∏´‡∏≤ logic_type ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
                best_match = None
                best_score = 0
                
                for logic_type, mapping in self.column_settings.items():
                    if not mapping:  # ‡∏Ç‡πâ‡∏≤‡∏° mapping ‡∏ó‡∏µ‡πà‡∏ß‡πà‡∏≤‡∏á
                        continue
                    
                    required_keys = set(self.normalize_col(c) for c in mapping.keys() if c)
                    required_vals = set(self.normalize_col(c) for c in mapping.values() if c)
                    
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô identity mapping ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                    is_identity_mapping = (required_keys == required_vals)
                    
                    if is_identity_mapping:
                        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö identity mapping ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡πÅ‡∏ö‡∏ö‡∏ï‡∏£‡∏á
                        match_count = len(header_row & required_keys)
                        total_required = len(required_keys)
                        
                        # ‡πÉ‡∏ä‡πâ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡πÅ‡∏ö‡∏ö‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß: ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 5 ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ï‡∏£‡∏á ‡∏´‡∏£‡∏∑‡∏≠ 10% ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö config ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà
                        if total_required > 0:
                            score = match_count / total_required
                            if total_required >= 50:
                                # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö config ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà (50+ ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå) ‡πÉ‡∏ä‡πâ‡πÄ‡∏Å‡∏ì‡∏ë‡πå 10% ‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 5 ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
                                min_threshold = max(0.1, 5/total_required)
                            elif total_required >= 20:
                                # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö config ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Å‡∏•‡∏≤‡∏á (20-49 ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå) ‡πÉ‡∏ä‡πâ‡πÄ‡∏Å‡∏ì‡∏ë‡πå 20% ‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 5 ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå  
                                min_threshold = max(0.2, 5/total_required)
                            else:
                                # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö config ‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å (< 20 ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå) ‡πÉ‡∏ä‡πâ‡πÄ‡∏Å‡∏ì‡∏ë‡πå 30%
                                min_threshold = 0.3
                            
                            if score >= min_threshold and score > best_score:
                                best_match = logic_type
                                best_score = score
                    else:
                        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö mapping ‡∏õ‡∏Å‡∏ï‡∏¥ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á keys ‡πÅ‡∏•‡∏∞ values
                        keys_match = len(header_row & required_keys)
                        vals_match = len(header_row & required_vals)
                        
                        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ match ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤
                        if keys_match > vals_match:
                            score = keys_match / len(required_keys) if required_keys else 0
                        else:
                            score = vals_match / len(required_vals) if required_vals else 0
                        
                        # ‡πÉ‡∏ä‡πâ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡πÅ‡∏ö‡∏ö‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
                        total_keys = len(required_keys) if keys_match > vals_match else len(required_vals)
                        if total_keys >= 50:
                            min_threshold = max(0.1, 5/total_keys)
                        elif total_keys >= 20:
                            min_threshold = max(0.2, 5/total_keys)
                        else:
                            min_threshold = 0.3
                            
                        if score >= min_threshold and score > best_score:
                            best_match = logic_type
                            best_score = score
                
                if best_match:
                    return best_match
            
            return None
        except Exception:
            return None

    def build_rename_mapping_for_dataframe(self, df_columns, logic_type):
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á mapping ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö df.rename(columns=...) ‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡πÉ‡∏´‡πâ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö identity mapping ‡πÅ‡∏•‡∏∞ mapping ‡∏õ‡∏Å‡∏ï‡∏¥
        - ‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÅ‡∏ö‡∏ö fuzzy matching ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
        """
        mapping = self.get_column_name_mapping(logic_type)
        if not mapping:
            return {}

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á normalized mapping ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
        df_cols_normalized = {self.normalize_col(c): c for c in df_columns}
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á mapping dictionaries
        keys_to_original = {self.normalize_col(k): k for k in mapping.keys() if k}
        vals_to_original = {self.normalize_col(v): v for v in mapping.values() if v}
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô identity mapping ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        normalized_keys = set(keys_to_original.keys())
        normalized_vals = set(vals_to_original.keys())
        is_identity_mapping = (normalized_keys == normalized_vals)
        
        if is_identity_mapping:
            # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö identity mapping ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á rename
            # ‡πÅ‡∏ï‡πà‡πÉ‡∏´‡πâ‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠ standardize
            result_mapping = {}
            
            for norm_col, original_df_col in df_cols_normalized.items():
                if norm_col in normalized_keys:
                    # ‡∏´‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô‡πÉ‡∏ô mapping
                    for orig_key, orig_val in mapping.items():
                        if self.normalize_col(orig_key) == norm_col:
                            # ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡∏à‡∏≤‡∏Å mapping ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÉ‡∏´‡∏°‡πà
                            if original_df_col != orig_key:
                                result_mapping[original_df_col] = orig_key
                            break
            
            return result_mapping
        else:
            # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö mapping ‡∏õ‡∏Å‡∏ï‡∏¥ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á
            keys_matches = set(df_cols_normalized.keys()) & normalized_keys
            vals_matches = set(df_cols_normalized.keys()) & normalized_vals
            
            result_mapping = {}
            
            if len(keys_matches) >= len(vals_matches):
                # DataFrame columns ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö keys ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ -> mapping ‡∏Ñ‡∏∑‡∏≠ old->new
                for norm_col, original_df_col in df_cols_normalized.items():
                    if norm_col in keys_to_original:
                        original_key = keys_to_original[norm_col]
                        new_value = mapping.get(original_key)
                        if new_value and original_df_col != new_value:
                            result_mapping[original_df_col] = new_value
            else:
                # DataFrame columns ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö values ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ -> mapping ‡∏ñ‡∏π‡∏Å‡πÉ‡∏™‡πà‡∏Å‡∏•‡∏±‡∏ö‡∏î‡πâ‡∏≤‡∏ô
                inverted_mapping = {v: k for k, v in mapping.items() if k and v}
                for norm_col, original_df_col in df_cols_normalized.items():
                    if norm_col in vals_to_original:
                        original_val = vals_to_original[norm_col]
                        new_value = inverted_mapping.get(original_val)
                        if new_value and original_df_col != new_value:
                            result_mapping[original_df_col] = new_value
            
            return result_mapping

    def debug_column_mapping(self, file_path, logic_type=None):
        """
        Debug ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£ mapping ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
        
        Args:
            file_path: ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏ü‡∏•‡πå
            logic_type: ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏à‡∏∞ auto detect)
            
        Returns:
            Dict: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• debug
        """
        try:
            # ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô (‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ log_callback ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏≠‡πà‡∏≤‡∏ô)
            original_log_callback = self.log_callback
            self.log_callback = lambda x: None  # Disable logging
            
            success, result = self.read_file_basic(file_path)
            
            # ‡∏Ñ‡∏∑‡∏ô log_callback ‡πÄ‡∏î‡∏¥‡∏°
            self.log_callback = original_log_callback
            
            if not success:
                return {"error": result}
            
            df = result
            actual_columns = list(df.columns)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏
            if not logic_type:
                logic_type = self.detect_file_type(file_path)
            
            # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• debug
            debug_info = {
                "file_path": file_path,
                "detected_logic_type": logic_type,
                "actual_columns": actual_columns,
                "actual_columns_count": len(actual_columns),
                "normalized_actual_columns": [self.normalize_col(c) for c in actual_columns],
            }
            
            if logic_type and logic_type in self.column_settings:
                mapping = self.column_settings[logic_type]
                debug_info.update({
                    "config_mapping": mapping,
                    "config_keys": list(mapping.keys()),
                    "config_values": list(mapping.values()),
                    "normalized_config_keys": [self.normalize_col(k) for k in mapping.keys()],
                    "normalized_config_values": [self.normalize_col(v) for v in mapping.values()],
                    "is_identity_mapping": set(self.normalize_col(k) for k in mapping.keys()) == set(self.normalize_col(v) for v in mapping.values()),
                })
                
                # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö rename mapping
                rename_mapping = self.build_rename_mapping_for_dataframe(df.columns, logic_type)
                debug_info["rename_mapping"] = rename_mapping
                debug_info["rename_mapping_count"] = len(rename_mapping)
                
                # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà
                actual_normalized = set(self.normalize_col(c) for c in actual_columns)
                config_keys_normalized = set(self.normalize_col(k) for k in mapping.keys())
                config_vals_normalized = set(self.normalize_col(v) for v in mapping.values())
                
                debug_info.update({
                    "keys_intersection": list(actual_normalized & config_keys_normalized),
                    "values_intersection": list(actual_normalized & config_vals_normalized),
                    "keys_match_count": len(actual_normalized & config_keys_normalized),
                    "values_match_count": len(actual_normalized & config_vals_normalized),
                    "missing_from_actual": list(config_keys_normalized - actual_normalized),
                    "extra_in_actual": list(actual_normalized - config_keys_normalized),
                })
            
            return debug_info
            
        except Exception as e:
            return {"error": f"Exception occurred: {type(e).__name__}: {str(e)}"}

    def read_file_basic(self, file_path, file_type='auto'):
        """
        ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå Excel (.xlsx, .xls) ‡∏´‡∏£‡∏∑‡∏≠ CSV ‡πÅ‡∏ö‡∏ö‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô (‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•)
        
        Args:
            file_path: ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏ü‡∏•‡πå
            file_type: ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå ('excel', 'excel_xls', 'csv', 'auto')
            
        Returns:
            Tuple[bool, Union[pd.DataFrame, str]]: (‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à, DataFrame ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î)
        """
        try:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if not os.path.exists(file_path):
                return False, f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå: {file_path}"
            
            # Auto-detect file type
            if file_type == 'auto':
                if file_path.lower().endswith('.csv'):
                    file_type = 'csv'
                elif file_path.lower().endswith('.xls'):
                    file_type = 'excel_xls'
                else:
                    file_type = 'excel'
            
            # ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå
            if file_type == 'csv':
                # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢: UTF-8 ‚Üí cp874 ‚Üí latin1
                try:
                    df = pd.read_csv(file_path, encoding='utf-8')
                except UnicodeDecodeError:
                    try:
                        df = pd.read_csv(file_path, encoding='cp874')
                    except UnicodeDecodeError:
                        df = pd.read_csv(file_path, encoding='latin1')
            elif file_type == 'excel_xls':
                # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå .xls ‡πÉ‡∏ä‡πâ xlrd engine
                df = pd.read_excel(file_path, sheet_name=0, engine='xlrd')
            else:
                # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå .xlsx
                df = pd.read_excel(file_path, sheet_name=0)
            
            if df.empty:
                return False, "‡πÑ‡∏ü‡∏•‡πå‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤"
            
            self.log_callback(f"‚úÖ ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {os.path.basename(file_path)} ({len(df):,} ‡πÅ‡∏ñ‡∏ß, {len(df.columns)} ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå)")
            
            return True, df
            
        except Exception as e:
            error_msg = f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå {os.path.basename(file_path)}: {str(e)}"
            self.log_callback(f"‚ùå {error_msg}")
            return False, error_msg

    def read_file_with_mapping(self, file_path, logic_type):
        """
        ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞ apply column mapping
        
        Args:
            file_path: ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏ü‡∏•‡πå
            logic_type: ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå‡∏ï‡∏≤‡∏° logic
            
        Returns:
            Tuple[bool, Union[pd.DataFrame, str]]: (‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à, DataFrame ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î)
        """
        try:
            # ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
            success, result = self.read_file_basic(file_path)
            if not success:
                return success, result
            
            df = result
            
            # Apply column mapping (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö header)
            col_map = self.build_rename_mapping_for_dataframe(df.columns, logic_type)
            if col_map:
                self.log_callback(f"üîÑ ‡∏õ‡∏£‡∏±‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ï‡∏≤‡∏° mapping ({len(col_map)} ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå)")
                df.rename(columns=col_map, inplace=True)
            
            return True, df
            
        except Exception as e:
            error_msg = f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• mapping ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {os.path.basename(file_path)}: {str(e)}"
            self.log_callback(f"‚ùå {error_msg}")
            return False, error_msg

    def peek_file_structure(self, file_path, num_rows=5):
        """
        ‡∏î‡∏π‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏≠‡πà‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        
        Args:
            file_path: ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏ü‡∏•‡πå
            num_rows: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏π
            
        Returns:
            Dict: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå
        """
        try:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if not os.path.exists(file_path):
                return {"error": f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå: {file_path}"}
            
            if file_path.lower().endswith('.csv'):
                file_type = 'csv'
            elif file_path.lower().endswith('.xls'):
                file_type = 'excel_xls'
            else:
                file_type = 'excel'
            
            # ‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏Ñ‡πà‡∏™‡πà‡∏ß‡∏ô‡∏ö‡∏ô
            if file_type == 'csv':
                df = pd.read_csv(file_path, nrows=num_rows, encoding='utf-8')
            elif file_type == 'excel_xls':
                df = pd.read_excel(file_path, sheet_name=0, nrows=num_rows, engine='xlrd')
            else:
                df = pd.read_excel(file_path, sheet_name=0, nrows=num_rows)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå
            detected_type = self.detect_file_type(file_path)
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô
            structure_info = {
                "file_name": os.path.basename(file_path),
                "file_type": file_type,
                "detected_logic_type": detected_type,
                "total_columns": len(df.columns),
                "columns": list(df.columns),
                "sample_data": df.to_dict('records'),
                "column_types": {col: str(df[col].dtype) for col in df.columns}
            }
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• mapping ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
            if detected_type:
                col_mapping = self.get_column_name_mapping(detected_type)
                if col_mapping:
                    structure_info["column_mapping"] = col_mapping
            
            return structure_info
            
        except Exception as e:
            return {"error": f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå: {str(e)}"}

    def get_file_info(self, file_path):
        """
        ‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå
        
        Args:
            file_path: ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏ü‡∏•‡πå
            
        Returns:
            Dict: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏ü‡∏•‡πå
        """
        try:
            if not os.path.exists(file_path):
                return {"error": f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå: {file_path}"}
            
            file_stats = os.stat(file_path)
            if file_path.lower().endswith('.csv'):
                file_type = 'csv'
            elif file_path.lower().endswith('.xls'):
                file_type = 'excel_xls'
            else:
                file_type = 'excel'
            
            # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡πÇ‡∏î‡∏¢‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà)
            try:
                if file_type == 'csv':
                    with open(file_path, 'r', encoding='utf-8') as f:
                        row_count = sum(1 for line in f) - 1  # ‡∏•‡∏ö header
                elif file_type == 'excel_xls':
                    # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Excel .xls ‡πÉ‡∏ä‡πâ xlrd engine
                    df_shape = pd.read_excel(file_path, sheet_name=0, engine='xlrd').shape
                    row_count = df_shape[0]
                else:
                    # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Excel .xlsx
                    df_shape = pd.read_excel(file_path, sheet_name=0).shape
                    row_count = df_shape[0]
            except:
                row_count = "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ô‡∏±‡∏ö‡πÑ‡∏î‡πâ"
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå
            detected_type = self.detect_file_type(file_path)
            
            return {
                "file_name": os.path.basename(file_path),
                "file_path": file_path,
                "file_size": f"{file_stats.st_size / (1024*1024):.2f} MB",
                "file_type": file_type,
                "detected_logic_type": detected_type or "‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å",
                "estimated_rows": row_count,
                "last_modified": pd.Timestamp.fromtimestamp(file_stats.st_mtime).strftime('%Y-%m-%d %H:%M:%S')
            }
            
        except Exception as e:
            return {"error": f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏ü‡∏•‡πå: {str(e)}"}

    def validate_file_before_processing(self, file_path, logic_type):
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏Å‡πà‡∏≠‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
        
        Args:
            file_path: ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏ü‡∏•‡πå
            logic_type: ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå‡∏ï‡∏≤‡∏° logic
            
        Returns:
            Dict: ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
        """
        validation_result = {
            "valid": False,
            "issues": [],
            "warnings": [],
            "file_info": {}
        }
        
        try:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if not os.path.exists(file_path):
                validation_result["issues"].append(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå: {file_path}")
                return validation_result
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå
            file_size = os.path.getsize(file_path)
            if file_size == 0:
                validation_result["issues"].append("‡πÑ‡∏ü‡∏•‡πå‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤")
                return validation_result
            
            if file_size > 100 * 1024 * 1024:  # 100 MB
                validation_result["warnings"].append("‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà (>100MB) ‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô")
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå
            detected_type = self.detect_file_type(file_path)
            if not detected_type:
                validation_result["warnings"].append("‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡πÑ‡∏î‡πâ")
            elif detected_type != logic_type:
                validation_result["warnings"].append(f"‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ ({detected_type}) ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏ ({logic_type})")
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
            structure = self.peek_file_structure(file_path, 1)
            if "error" in structure:
                validation_result["issues"].append(structure["error"])
                return validation_result
            
            validation_result["file_info"] = structure
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
            if logic_type in self.column_settings:
                required_original_cols = set(self.column_settings[logic_type].keys())
                file_cols = set(structure["columns"])
                
                # Normalize ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
                required_normalized = set(self.normalize_col(col) for col in required_original_cols)
                file_normalized = set(self.normalize_col(col) for col in file_cols)
                
                missing_cols = required_normalized - file_normalized
                if missing_cols:
                    validation_result["issues"].append(f"‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢‡πÑ‡∏õ: {missing_cols}")
                else:
                    validation_result["valid"] = True
            else:
                validation_result["warnings"].append(f"‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå '{logic_type}'")
                validation_result["valid"] = True  # ‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ config
            
        except Exception as e:
            validation_result["issues"].append(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö: {str(e)}")
        
        return validation_result

    def list_available_file_types(self):
        """‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏î‡πâ"""
        if not self.column_settings:
            return []
        
        file_types = []
        for logic_type, mapping in self.column_settings.items():
            if isinstance(mapping, dict) and mapping:
                file_types.append({
                    "logic_type": logic_type,
                    "required_columns": len(mapping),
                    "column_mapping": mapping
                })
        
        return file_types
