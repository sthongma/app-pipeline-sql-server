"""
File Service for PIPELINE_SQLSERVER (Reorganized New Version)

Orchestrator that combines various services:
- FileReaderService: Read and detect files
- DataProcessorService: Process and validate data
- FileManagementService: Manage files

Usage examples:
    # For GUI
    file_service = FileService(log_callback=gui_log_function)
    
    # For CLI
    file_service = FileService(log_callback=logging.info)
    
    # Normal usage (original interface)
    success, df = file_service.read_excel_file("data.xlsx", "sales_data")
    
    # Separate usage
    file_info = file_service.get_file_info("data.xlsx")
    validation = file_service.validate_file_before_processing("data.xlsx", "sales_data")
"""

from typing import Optional, Tuple
import logging

from services.file import (
    FileReaderService,
    DataProcessorService,
    FileManagementService
)
from performance_optimizations import PerformanceOptimizer
from config.json_manager import load_column_settings, load_dtype_settings


class FileOrchestrator:
    """
    Main file service (orchestrator)
    
    Responsibilities:
    - Coordinate between different services
    - Complete file reading and processing
    - Provide same interface as legacy system
    """
    
    def __init__(self, search_path: Optional[str] = None, log_callback: Optional[callable] = None) -> None:
        """
        Initialize FileService
        
        Args:
            search_path (Optional[str]): Folder path for file search
            log_callback (Optional[callable]): Function for logging
        """
        self.log_callback = log_callback if log_callback else logging.info
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á services
        self.file_reader = FileReaderService(search_path, self.log_callback)
        self.data_processor = DataProcessorService(self.log_callback)
        self.file_manager = FileManagementService(search_path)
        
        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å JSON Manager
        # Note: Settings are now managed by SettingsManager singleton
        # These assignments trigger the property setters which save to SettingsManager
        column_settings = load_column_settings()
        dtype_settings = load_dtype_settings()

        self.file_reader.column_settings = column_settings
        self.file_reader.dtype_settings = dtype_settings

        self.data_processor.column_settings = column_settings
        self.data_processor.dtype_settings = dtype_settings
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á performance optimizer
        self.performance_optimizer = PerformanceOptimizer(self.log_callback)
        
        # ‡πÄ‡∏Å‡πá‡∏ö reference ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö backward compatibility
        self.search_path = self.file_reader.search_path
        self.column_settings = column_settings

    # ========================
    # Main Interface Methods
    # ========================
    
    def preview_file_columns(self, file_path, logic_type, max_rows=5):
        """
        Check file columns by reading only the first few rows to save time
        
        Args:
            file_path: File path
            logic_type: File type
            max_rows: Number of rows to read for preview (default: 5)
            
        Returns:
            tuple: (success, result/error_message, columns_info)
        """
        try:
            import pandas as pd
            
            # ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
            if file_path.lower().endswith('.csv'):
                try:
                    preview_df = pd.read_csv(file_path, nrows=max_rows, encoding='utf-8')
                except UnicodeDecodeError:
                    preview_df = pd.read_csv(file_path, nrows=max_rows, encoding='tis-620')
            else:
                # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Excel
                preview_df = pd.read_excel(file_path, nrows=max_rows)
            
            if preview_df.empty:
                return False, "File is empty", None
            
            # ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏à‡∏≤‡∏Å preview
            file_columns = list(preview_df.columns)
            
            # Apply column mapping ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ß‡πà‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏à‡∏∞‡∏ñ‡∏π‡∏Å rename ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£
            col_map = self.file_reader.build_rename_mapping_for_dataframe(file_columns, logic_type)
            
            mapped_columns = file_columns.copy()
            if col_map:
                for old_name, new_name in col_map.items():
                    if old_name in mapped_columns:
                        idx = mapped_columns.index(old_name)
                        mapped_columns[idx] = new_name
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
            success, validation_message = self.data_processor.validate_columns_by_list(mapped_columns, logic_type)
            
            columns_info = {
                'original_columns': file_columns,
                'mapped_columns': mapped_columns,
                'column_mapping': col_map,
                'total_columns': len(file_columns),
                'preview_rows': len(preview_df)
            }
            
            if success:
                return True, f"Columns validation passed for {logic_type}", columns_info
            else:
                return False, validation_message, columns_info
                
        except Exception as e:
            return False, f"Error previewing file: {str(e)}", None

    def read_excel_file(self, file_path, logic_type):
        """
        Read Excel or CSV file according to specified type without using automatic correction system
        
        Args:
            file_path: File path
            logic_type: File type
            No automatic correction system is used
        """
        try:
            # ‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï log flags ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà
            self.data_processor._reset_log_flags()
            
            # ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏î‡πâ‡∏ß‡∏¢ Performance Optimizer
            if file_path.lower().endswith('.csv'):
                file_type = 'csv'
            elif file_path.lower().endswith('.xls'):
                file_type = 'excel_xls'
            else:
                file_type = 'excel'
            
            success, df = self.performance_optimizer.read_large_file_chunked(file_path, file_type)
            if not success:
                return False, "Unable to read file"
            
            # Apply column mapping (‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á mapping ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö header ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå)
            col_map = self.file_reader.build_rename_mapping_for_dataframe(df.columns, logic_type)
            if col_map:
                self.log_callback(f"üîÑ Renamed columns by mapping ({len(col_map)} columns)")
                df.rename(columns=col_map, inplace=True)
            
            # ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á memory usage
            df = self.performance_optimizer.optimize_memory_usage(df)
            
            # ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏ô staging table ‡∏î‡πâ‡∏ß‡∏¢ SQL ‡πÅ‡∏ó‡∏ô pandas
            self.log_callback(f"üîÑ Ingest as NVARCHAR(MAX) first, then validate/convert using SQL")
            
            # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î memory
            self.performance_optimizer.cleanup_memory()
            
            self.log_callback(f"üéâ File processing completed")
            return True, df
            
        except Exception as e:
            error_msg = f"‚ùå Error while reading file: {e}"
            self.log_callback(error_msg)
            return False, error_msg
    
    # ========================
    # Delegation Methods
    # ========================
    
    def set_search_path(self, path):
        """Set path for Excel file search"""
        self.search_path = path
        self.file_reader.set_search_path(path)

    def find_data_files(self):
        """Find Excel and CSV files in specified path"""
        return self.file_reader.find_data_files()

    def detect_file_type(self, file_path):
        """Detect file type"""
        return self.file_reader.detect_file_type(file_path)

    def get_column_name_mapping(self, file_type):
        """Get column name mapping by file type"""
        return self.file_reader.get_column_name_mapping(file_type)

    def get_required_dtypes(self, file_type):
        """Get column dtypes by file type"""
        return self.data_processor.get_required_dtypes(file_type)

    def validate_columns(self, df, logic_type):
        """Validate required columns"""
        return self.data_processor.validate_columns(df, logic_type)

    def comprehensive_data_validation(self, df, logic_type):
        """Comprehensive data validation before processing"""
        return self.data_processor.comprehensive_data_validation(df, logic_type)

    def check_invalid_numeric(self, df, logic_type):
        """Check non-numeric values in numeric columns"""
        return self.data_processor.check_invalid_numeric(df, logic_type)

    def generate_pre_processing_report(self, df, logic_type):
        """Generate pre-processing summary report (deprecated - use SQL validation instead)"""
        return self.data_processor.generate_pre_processing_report(df, logic_type)

    def apply_dtypes(self, df, file_type):
        """Convert data types according to settings"""
        return self.data_processor.apply_dtypes(df, file_type)

    def move_uploaded_files(self, file_paths, logic_types=None):
        """Move uploaded files to Uploaded_Files folder"""
        return self.file_manager.move_uploaded_files(file_paths, logic_types, self.search_path)

    # ========================
    # Legacy Methods (‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠ backward compatibility)
    # ========================
    
    def get_required_columns(self, file_type):
        """(Deprecated) Use get_required_dtypes instead"""
        return self.data_processor.get_required_dtypes(file_type)

    def standardize_column_name(self, col_name):
        """Convert column names to standard format"""
        return self.file_reader.standardize_column_name(col_name)

    def normalize_col(self, col):
        """Improve column normalization"""
        return self.file_reader.normalize_col(col)

    def load_settings(self):
        """Load new settings from JSON Manager"""
        # ‡πÇ‡∏´‡∏•‡∏î‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å JSON Manager
        column_settings = load_column_settings()
        dtype_settings = load_dtype_settings()
        
        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô file_reader ‡πÅ‡∏•‡∏∞ data_processor
        self.file_reader.column_settings = column_settings
        self.file_reader.dtype_settings = dtype_settings
        self.file_reader._settings_loaded = True
        
        self.data_processor.column_settings = column_settings  
        self.data_processor.dtype_settings = dtype_settings
        self.data_processor._settings_loaded = True
        
        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï reference ‡πÉ‡∏ô FileService
        self.column_settings = column_settings

    def _process_dataframe_in_chunks(self, df, process_func, logic_type, chunk_size=5000):
        """Process DataFrame in chunks (legacy wrapper)"""
        return self.data_processor.process_dataframe_in_chunks(df, process_func, logic_type, chunk_size)

    def _reset_log_flags(self):
        """Reset log flags (legacy wrapper)"""
        self.data_processor._reset_log_flags()

    def clean_numeric_columns(self, df, file_type):
        """Clean numeric column data"""
        return self.data_processor.clean_numeric_columns(df, file_type)

    def truncate_long_strings(self, df, logic_type):
        """Truncate strings that exceed specified length"""
        return self.data_processor.truncate_long_strings(df, logic_type)


    def upload_data_with_auto_schema_update(self, df, logic_type, processing_report=None, schema_name='bronze'):
        """
        Upload data with automatic schema update
        
        Args:
            df: DataFrame to upload
            logic_type: File type
            processing_report: (Not used)
            schema_name: Schema name in database
            
        Returns:
            Tuple[bool, str]: (Success status, Result message)
        """
        try:
            from services.orchestrators.database_orchestrator import DatabaseOrchestrator
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á database service
            db_service = DatabaseOrchestrator()
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠
            connection_ok, conn_msg = db_service.check_connection()
            if not connection_ok:
                return False, f"Unable to connect to database: {conn_msg}"
            
            # ‡πÑ‡∏î‡πâ required columns ‡∏ï‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
            required_cols = self.get_required_dtypes(logic_type)
            if not required_cols:
                return False, "Data type settings not found"
            
            # ‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö auto-fix ‡∏≠‡∏µ‡∏Å‡∏ï‡πà‡∏≠‡πÑ‡∏õ ‡∏à‡∏∂‡∏á‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ force_recreate ‡∏à‡∏≤‡∏Å processing_report
            force_recreate = False
            
            # ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            success, upload_msg = db_service.upload_data(
                df=df,
                logic_type=logic_type,
                required_cols=required_cols,
                schema_name=schema_name,
                log_func=self.log_callback,
                force_recreate=force_recreate
            )
            
            return success, upload_msg
            
        except Exception as e:
            error_msg = f"‚ùå Error during upload: {e}"
            self.log_callback(error_msg)
            return False, error_msg


    def print_detailed_validation_report(self, df, logic_type):
        """Display detailed data validation report (legacy method)"""
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
        self.log_callback("\n" + "="*80)
        self.log_callback("üîç Detailed data validation report")
        self.log_callback("="*80)
        
        # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        self.log_callback(f"üìä Basic info:")
        self.log_callback(f"   ‚Ä¢ Total rows: {len(df):,}")
        self.log_callback(f"   ‚Ä¢ Total columns: {len(df.columns)}")
        self.log_callback(f"   ‚Ä¢ File type: {logic_type}")
        
        # ‡πÉ‡∏ä‡πâ comprehensive validation
        validation_result = self.comprehensive_data_validation(df, logic_type)
        
        if validation_result['summary']:
            self.log_callback("\nüìã Validation summary:")
            for msg in validation_result['summary']:
                self.log_callback(f"   ‚Ä¢ {msg}")
        
        self.log_callback("="*80)
